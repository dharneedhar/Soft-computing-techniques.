
__init__.py


# Copyright 2016-2020 Fabian Hofmann (FIAS), Jonas Hoersch (KIT, IAI) and
# Fabian Gotzens (FZJ, IEK-STE)

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 3 of the
# License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

            
                        HYDRO POWER PLANT........
    
    
A set of tools for cleaning, standardising and combining multiple
power plant databases.


from __future__ import absolute_import

__version__ = "0.4.5"
__author__ = "Fabian Hofmann, Jonas Hoersch, Fabian Gotzens"
__copyright__ = "Copyright 2017-2020 Frankfurt Institute for Advanced Studies"
#The rough hierarchy of this package is
#core, utils, heuristics, cleaning, matching, collection, data

# Commonly used sub-modules. Imported here to provide end-user
# convenience.
from . import core
from . import utils
from . import heuristics
from . import data
#from . import cleaning
#from . import matching
#from . import collection
from . import plot
from .core import get_config
from .collection import matched_data as powerplants
from .accessor import PowerPlantAccessor


//////////////accessor.py///////////////////



import pandas


@pandas.api.extensions.register_dataframe_accessor("powerplant")
class PowerPlantAccessor():
    
    #//Accessor object for DataFrames created with powerplantmatching.
   # This simplifies the access to common functions applicable to dataframes
    #with powerplant data. Note even though this is a general DataFrame
    #accessor, the functions will only work for powerplantmatching related
    DataFrames.
  
    import powerplantmatching as pm
    entsoe = pm.data.ENTSOE()
    entsoe.powerplant.plot_aggregated()
 

    def __init__(self, pandas_obj):
        self._obj = pandas_obj

    from .plot import powerplant_map as plot_map
    from .utils import (lookup, set_uncommon_fueltypes_to_other,
                        select_by_projectID, breakdown_matches,
                        fill_geoposition, convert_country_to_alpha2,
                        convert_alpha2_to_country)
    from .export import to_pypsa_names
    from .heuristics import ( extend_by_non_matched, scale_to_net_capacities,
        fill_missing_commyears, extend_by_VRE, fill_missing_duration,
        rescale_capacities_to_country_totals, fill_missing_decommyears)
    
    from .cleaning import clean_powerplantname, aggregate_units
    from .matching import reduce_matched_dataframe

    def plot_aggregated(self, by=['Country', 'Fueltype'], figsize=(12, 20),
                        **kwargs):
        
        Plotting function for fast inspection of the capacity distribution.
        Returns figure and axes of a matplotlib barplot.
        
        by : list, default ['Country', 'Fueltype']
            Define the columns of the dataframe to be grouped on.
        figsize : tuple, default (12,20)
            width and height of the figure
        #key word args:
            keywordargument for matplotlib plotting
      
        import matplotlib.pyplot as plt
        from .utils import lookup, convert_country_to_alpha2
        subplots = True if len(by) > 1 else False
        fig, ax = plt.subplots(figsize=figsize, **kwargs)
        df = lookup(convert_country_to_alpha2(self._obj), by=by)
        df = df.unstack().rename_axis(None) if subplots else df
        df.plot.bar(subplots=subplots, sharex=False, ax=ax)
        fig.tight_layout(h_pad=1.)
        return fig, ax

    def set_name(self, name):
        self._obj.columns.name = name

    def get_name(self):
        return self._obj.columns.name

    def match_with(self, df, labels=None, use_saved_matches=False,
                   config=None, reduced=True, **dukeargs):
        from .matching import (combine_multiple_datasets,
                               reduce_matched_dataframe)
        from .utils import to_list_if_other

        dfs = [self._obj] + to_list_if_other(df)
        res = combine_multiple_datasets(
            dfs, labels, use_saved_matches=use_saved_matches, config=config,
            **dukeargs)
        if reduced:
            return res.pipe(reduce_matched_dataframe, config=config)
        return res
    pass



/////////////////////////////cleaning.py///////////////////////


  Functions for vertically cleaning a dataset.

    from __future__ import absolute_import, print_function

from .core import get_config, _data_out, get_obj_if_Acc
from .utils import get_name, set_column_name
from .duke import duke

import os
import numpy as np
import pandas as pd
import networkx as nx
import logging
logger = logging.getLogger(__name__)


def clean_powerplantname(df):
   
   # Cleans the column "Name" of the database by deleting very frequent
    #words, numericals and nonalphanumerical characters of the
    #column. Returns a reduced dataframe with nonempty Name-column.
    
    
    df : pandas.Dataframe
        dataframe to be cleaned
  
    df = get_obj_if_Acc(df)
    df = df[df.Name.notnull()]
    name = df.Name.replace(regex=True, value=' ',
                           to_replace=['-', '/', ',', '\(', '\)', '\[', '\]',
                                       '"', '_', '\+', '[0-9]'])

    common_words = pd.Series(sum(name.str.split(), [])).value_counts()
    cw = list(common_words[common_words >= 20].index)

    pattern = [('(?i)(^|\s)'+x+'(?=\s|$)')
               for x in (cw +
                         ['[a-z]', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII',
                          'VIII', 'IX', 'X', 'XI', 'Grupo', 'parque', 'eolico',
                          'gas', 'biomasa', 'COGENERACION', 'gt', 'unnamed',
                          'tratamiento de purines', 'planta', 'de', 'la',
                          'station', 'power', 'storage', 'plant', 'stage',
                          'pumped', 'project', 'dt', 'gud', 'hkw', 'kbr',
                          'Kernkraft', 'Kernkraftwerk', 'kwg', 'krb', 'ohu',
                          'gkn', 'Gemeinschaftskernkraftwerk', 'kki', 'kkp',
                          'kle', 'wkw', 'rwe', 'bis', 'nordsee', 'ostsee',
                          'dampfturbinenanlage', 'ikw', 'kw', 'kohlekraftwerk',
                          'raffineriekraftwerk', 'Kraftwerke'])]
    name = (name
            .replace(regex=True, to_replace=pattern, value=' ')
            .replace('\s+', ' ', regex=True)
            .replace('"', '', regex=True)
            .str.strip()
            .str.capitalize())

    return (df
            .assign(Name=name)
            .loc[lambda x: x.Name != '']
            .sort_values('Name')
            .reset_index(drop=True))


def gather_fueltype_info(df, search_col=['Name', 'Technology']):

    
    search_col : list, default is ['Name', 'Technology']
        Specify the columns to be parsed
    
    fueltype = pd.Series(df['Fueltype'])

    for i in search_col:
        found_b = df[i].dropna().str.contains('(?i)lignite|brown')
        fueltype.loc[found_b.reindex(fueltype.index,
                                     fill_value=False)] = 'Lignite'
    fueltype.replace({'Coal': 'Hard Coal'}, inplace=True)

    return df.assign(Fueltype=fueltype)


def gather_technology_info(df, search_col=['Name', 'Fueltype'],
                           config=None):
   
   # Parses in search_col columns for distinct technology specifications, e.g.
    #'Run-of-River', and passes this information to the 'Technology' column.
   
    search_col : list, default is ['Name', 'Fueltype']
        Specify the columns to be parsed
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
   
    if config is None:
        config = get_config()

    technology = (df['Technology'].dropna()
                  if 'Technology' in df
                  else pd.Series())

    pattern = '|'.join(('(?i)'+x) for x in config['target_technologies'])
    for i in search_col:
        found = (df[i].dropna()
                 .str.findall(pattern)
                 .loc[lambda s: s.str.len() > 0]
                 .str.join(sep=', '))

        exists_i = technology.index.intersection(found.index)
        if len(exists_i) > 0:
            technology.loc[exists_i] = (technology.loc[exists_i].str
                                        .cat(found.loc[exists_i], sep=', '))

        new_i = found.index.difference(technology.index)
        technology = technology.append(found[new_i])

    return df.assign(Technology=technology)


def gather_set_info(df, search_col=['Name', 'Fueltype', 'Technology']):
    
    
    #Parses in search_col columns for distinct set specifications, e.g.
    #'Store', and passes this information to the 'Set' column.
   
    search_col : list, default is ['Name', 'Fueltype', 'Technology']
        Specify the columns to be parsed
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
   
    Set = (df['Set'].copy()
           if 'Set' in df
           else pd.Series(index=df.index))

    pattern = '|'.join(['heizkraftwerk', 'hkw', 'chp', 'bhkw', 'cogeneration',
                        'power and heat', 'heat and power'])
    for i in search_col:
        isCHP_b = df[i].dropna().str.contains(pattern, case=False)\
                    .reindex(df.index).fillna(False)
        Set.loc[isCHP_b] = 'CHP'

    pattern = '|'.join(['battery', 'storage'])
    for i in search_col:
        isStore_b = df[i].dropna().str.contains(pattern, case=False) \
                    .reindex(df.index).fillna(False)
        Set.loc[isStore_b] = 'Store'

    df = df.assign(Set=Set)
    df.loc[:, 'Set'].fillna('PP', inplace=True)
    return df


def clean_technology(df, generalize_hydros=False):
    
   # Clean the 'Technology' by condensing down the value into one claim. This
   # procedure might reduce the scope of information, however is crucial for
    #comparing different data sources.
   
    search_col : list, default is ['Name', 'Fueltype', 'Technology']
        Specify the columns to be parsed
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
    
    tech = df['Technology'].dropna()
    if len(tech) == 0:
        return df
    tech = tech.replace(
            {' and ': ', ', ' Power Plant': '', 'Battery': ''}, regex=True)
    if generalize_hydros:
        tech[tech.str.contains('pump', case=False)] = 'Pumped Storage'
        tech[tech.str.contains('reservoir|lake', case=False)] = 'Reservoir'
        tech[tech.str.contains('run-of-river|weir|water', case=False)] =\
            'Run-Of-River'
        tech[tech.str.contains('dam', case=False)] = 'Reservoir'
    tech = tech.replace({'Gas turbine': 'OCGT'})
    tech[tech.str.contains('combined cycle|combustion', case=False)] = 'CCGT'
    tech[tech.str.contains('steam turbine|critical thermal', case=False)] =\
        'Steam Turbine'
    tech[tech.str.contains('ocgt|open cycle', case=False)] = 'OCGT'
    tech = (tech.str.title()
                .str.split(', ')
                .apply(lambda x: ', '.join(i.strip() for i in np.unique(x))))
    tech = tech.replace({'Ccgt': 'CCGT', 'Ocgt': 'OCGT'}, regex=True)
    return df.assign(Technology=tech)


def cliques(df, dataduplicates):
    
   # Locate cliques of units which are determined to belong to the same
    #powerplant.  Return the same dataframe with an additional column
    #"grouped" which indicates the group that the powerplant is
    #belonging to.
   
    df : pandas.Dataframe or string
        dataframe or csv-file which should be analysed
    dataduplicates : pandas.Dataframe or string
        dataframe or name of the csv-linkfile which determines the
        link within one dataset
     
    df = read_csv_if_string(df)
    G = nx.DiGraph()
    G.add_nodes_from(df.index)
    G.add_edges_from((r.one, r.two) for r in dataduplicates.itertuples())
    H = G.to_undirected(reciprocal=True)

    grouped = pd.Series(np.nan, index=df.index)
    for i, inds in enumerate(nx.algorithms.clique.find_cliques(H)):
        grouped.loc[inds] = i

    return df.assign(grouped=grouped)


def aggregate_units(df, dataset_name=None,
                    pre_clean_name=True,
                    save_aggregation=True,
                    country_wise=True,
                    use_saved_aggregation=False,
                    config=None):

 # Vertical cleaning of the database. Cleans the "Name"-column, sums up the capacity of powerplant units which are determined to belongto the same plant.
  
    df : pandas.Dataframe or string
        Dataframe or name to use for the resulting database
    dataset_name : str, default None
        Specify the name of your df, required if use_saved_aggregation is set
        to True.
    pre_clean_name : Boolean, default True
        Whether to clean the 'Name'-column before aggregating.
    use_saved_aggregation : bool (default False):
        Whether to use the automaticly saved aggregation file, which
        is stored in data/out/default/aggregations/aggregation_groups_XX.csv
        with XX being the name for the dataset. This saves time if you
        want to have aggregated powerplants without running the
        aggregation algorithm again
    
    df = get_obj_if_Acc(df)

    if config is None:
        config = get_config()

    weighted_cols = [col for col in ['Efficiency', 'Duration']
                     if col in config['target_columns']]
    df = (df.assign(**{col: df[col] * df.Capacity for col in weighted_cols})
            .assign(lat=df.lat.astype(float),
                    lon=df.lon.astype(float))
            .assign(**{col: df[col].astype(str) for col in
                       ['Name', 'Country', 'Fueltype',
                        'Technology', 'Set', 'File']
                       if col in config['target_columns']}))

    def mode(x):
        return x.mode(dropna=False).at[0]

    props_for_groups = pd.Series(
        {'Name': mode,
         'Fueltype': mode,
         'Technology': mode,
         'Set': mode,
         'Country': mode,
         'Capacity': 'sum',
         'lat': 'mean',
         'lon': 'mean',
         'DateIn': 'min',
         'DateRetrofit': 'max',  # choose latest Retrofit-Year
         'DateMothball': 'min',
         'DateOut': 'min',
         'File': mode,
         'projectID': list,
         'EIC': set,
         'Duration': 'sum',  # note this is weighted sum
         'Volume_Mm3': 'sum',
         'DamHeight_m': 'sum',
         'StorageCapacity_MWh': 'sum',
         'Efficiency': 'mean'  # note this is weighted mean
         }).reindex(config['target_columns'], axis=1).to_dict()

    dataset_name = get_name(df) if dataset_name is None else dataset_name

    if pre_clean_name:
        df = clean_powerplantname(df)

    logger.info("Aggregating blocks to entire units in '{}'."
                .format(dataset_name))

    path_name = _data_out('aggregations/aggregation_groups_{}.csv'
                          .format(dataset_name), config=config)


    if use_saved_aggregation & save_aggregation:
        if os.path.exists(path_name):
            logger.info("Reading saved aggregation groups for dataset '{}'."
                        .format(dataset_name))
            groups = (pd.read_csv(path_name, header=None, index_col=0)
                        .reindex(index=df.index))
            df = df.assign(grouped=groups.values)
        else:
            logger.info(f"No existing saved aggregation groups for dataset "
                        f"'{dataset_name}', continuing by aggregating again.")
            if 'grouped' in df:
                df.drop('grouped', axis=1, inplace=True)
    else:
        logger.info(f"Not using saved aggregation groups for dataset "
                    f"'{dataset_name}'.")

    if 'grouped' not in df:
        if country_wise:
            duplicates = pd.concat([duke(df.query('Country == @c'))
                                    for c in df.Country.unique()])
        else:
            duplicates = duke(df)
        df = cliques(df, duplicates)
        if save_aggregation:
            df.grouped.to_csv(path_name, header=False)

    df = df.groupby('grouped').agg(props_for_groups)
    df = df.replace('nan', np.nan)

    if 'EIC' in df:
        df = df.assign(EIC=df['EIC'].apply(list))

    df = (df
          .assign(**{col: df[col].div(df['Capacity']) 
                     for col in weighted_cols})
          .reset_index(drop=True)
          .pipe(clean_powerplantname)
          .reindex(columns=config['target_columns'])
          .pipe(set_column_name, dataset_name))
    return df


////////////////////collection.py/////////////////////////


Processed datasets of merged and/or adjusted data

from __future__ import print_function

from .core import _data_out, get_config
from .utils import (set_uncommon_fueltypes_to_other, parmap,
                    to_dict_if_string, projectID_to_dict, set_column_name)
from .heuristics import (extend_by_non_matched, extend_by_VRE)
from .cleaning import aggregate_units
from .matching import combine_multiple_datasets, reduce_matched_dataframe

import pandas as pd
import os
import logging
logger = logging.getLogger(__name__)


def collect(datasets, update=False, use_saved_aggregation=True,
            use_saved_matches=True, reduced=True,
            custom_config={}, config=None, **dukeargs):
    
    Return the collection for a given list of datasets in matched or
    reduced form.
    
    datasets : list or str
        list containing the dataset identifiers as str, or single str
    update : bool
        Do an horizontal update (True) or read from the cache file (False)
    use_saved_aggregation : bool
        Aggregate units based on cached aggregation group files (True)
        or to do an vertical update (False)
    use_saved_matches : bool
        Match datasets based on cached matched pair files (True)
        or to do an horizontal matching (False)
    reduced : bool
        Switch as to return the reduced (True) or matched (False) dataset.
    custom_config : dict
        Updates to the data_config dict from data module
    
    dukeargs : keyword-args for duke
    

    from . import data
    if config is None:
        config = get_config()

    def df_by_name(name):
        conf = config[name]

        get_df = getattr(data, name)
        df = get_df(config=config, **conf.get('read_kwargs', {}))
        if not conf.get('aggregated_units', False):
            return aggregate_units(df,
                                   use_saved_aggregation=use_saved_aggregation,
                                   dataset_name=name,
                                   config=config)
        else:
            return df.assign(projectID=df.projectID.map(lambda x: [x]))

    # Deal with the case that only one dataset is requested
    if isinstance(datasets, str):
        return df_by_name(datasets)

    datasets = sorted(datasets)
    logger.info('Collect combined dataset for {}'.format(', '.join(datasets)))
    outfn_matched = _data_out('Matched_{}.csv'
                              .format('_'.join(map(str.upper, datasets))),
                              config=config)
    outfn_reduced = _data_out('Matched_{}_reduced.csv'
                              .format('_'.join(map(str.upper, datasets))),
                              config=config)

    if not update and not os.path.exists(outfn_reduced
                                         if reduced else outfn_matched):
        logger.warning("Forcing update since the cache file is missing")
        update = True
        use_saved_aggregation = True

    if update:
        dfs = parmap(df_by_name, datasets)
        matched = combine_multiple_datasets(
            dfs, datasets, use_saved_matches=use_saved_matches, config=config,
            **dukeargs)
        (matched.assign(projectID=lambda df: df.projectID.astype(str))
                .to_csv(outfn_matched, index_label='id'))

        reduced_df = reduce_matched_dataframe(matched, config=config)
        reduced_df.to_csv(outfn_reduced, index_label='id')

        return reduced_df if reduced else matched
    else:
        if reduced:
            df = pd.read_csv(outfn_reduced, index_col=0)
        else:
            df = pd.read_csv(outfn_matched, index_col=0, header=[0, 1],
                             low_memory=False)
        return df.pipe(projectID_to_dict)


def Collection(**kwargs):
    return collect(**kwargs)


def matched_data(config=None,
                 stored=True,
                 update=False,
                 update_all=False,
                 from_url=False,
                 extend_by_vres=False,
                 extendby_kwargs={'use_saved_aggregation': True},
                 subsume_uncommon_fueltypes=False,
                 **collection_kwargs):
  
    
    stored : Boolean, default True
            Whether to use the stored matched_data.csv file in data/out/default
            If False, the matched data is taken from collect() and
            extended afterwards. To update the whole matching, please set
            stored=False and update=True.
    update : Boolean, default False
            Whether to rerun the matching process. Overrides stored to False
            if True.
    update_all : Boolean, default False
            Whether to rerun the matching process and aggregation process.
            Overrides stored to False if True.
    from_url: Boolean, default False
            Whether to parse and store the already build data from the repo
            website.
    config : Dict, default None
            Define a configuration varying from the setting in config.yaml.
            Relevant keywords are 'matching_sources', 'fully_included_sources'.
    extend_by_vres : Boolean, default False
            Whether extend the dataset by variable renewable energy sources
            given by powerplantmatching.data.OPSD_VRE()
    extendby_kwargs : Dict, default {'use_saved_aggregation': True}
            Dict of keywordarguments passed to powerplantmatchting.
            heuristics.extend_by_non_matched
    subsume_uncommon_fueltypes : Boolean, default False
            Whether to replace uncommon fueltype specification by 'Other'
    **collection_kwargs : kwargs
            Arguments passed to powerplantmatching.collection.Collection.
            Typical arguments are update, use_saved_aggregation,
            use_saved_matches.
            
    if config is None:
        config = get_config()

    if update_all:
        collection_kwargs['use_saved_aggregation'] = False
        collection_kwargs['use_saved_matches'] = False
        update = True
    if update:
        stored = False

    collection_kwargs.setdefault('update', update)

    if collection_kwargs.get('reduced', True):
        fn = _data_out('matched_data_red.csv')
        header = 0
    else:
        fn = _data_out('matched_data.csv')
        header = [0, 1]

    if from_url:
        fn = _data_out('matched_data_red.csv')
        url = config['matched_data_url']
        logger.info(f'Retrieving data from {url}')
        df = (pd.read_csv(url, index_col=0)
                .pipe(projectID_to_dict)
                .pipe(set_column_name, 'Matched Data'))
        logger.info(f'Store data at {fn}')
        df.to_csv(fn)
        return df

    if stored and os.path.exists(fn):
        df = (pd.read_csv(fn, index_col=0, header=header)
                .pipe(projectID_to_dict)
                .pipe(set_column_name, 'Matched Data'))
        if extend_by_vres:
            return df.pipe(extend_by_VRE, config=config,
                           base_year=config['opsd_vres_base_year'])
        return df

    matching_sources = [list(to_dict_if_string(a))[0]
                        for a in config['matching_sources']]
    matched = collect(matching_sources, **collection_kwargs)

    if isinstance(config['fully_included_sources'], list):
        for source in config['fully_included_sources']:
            source = to_dict_if_string(source)
            name, = list(source)
            extendby_kwargs.update({'query': source[name]})
            matched = extend_by_non_matched(matched, name, config=config,
                                            **extendby_kwargs)

    # Drop matches between only low reliability-data, this is necessary since
    # a lot of those are decommissioned, however some countries only appear in
    # GEO and CARMA
    
    allowed_countries = config['CARMA_GEO_countries']
    if matched.columns.nlevels > 1:
        other = set(matching_sources) - set(['CARMA', 'GEO'])
        matched = (matched[~matched.projectID[other].isna().all(1)
                           | matched.Country.GEO.isin(allowed_countries)
                           | matched.Country.CARMA.isin(allowed_countries)]
                   .reset_index(drop=True))
        if config['remove_missing_coords']:
            matched = (matched[matched.lat.notnull().any(1)]
                       .reset_index(drop=True))
    else:
        matched = (matched[matched.projectID.apply(lambda x: sorted(x.keys())
                           not in [['CARMA', 'GEO']])
                           | matched.Country.isin(allowed_countries)]
                   .reset_index(drop=True))
        if config['remove_missing_coords']:
            matched = matched[matched.lat.notnull()].reset_index(drop=True)
    matched.to_csv(fn, index_label='id', encoding='utf-8')

    if extend_by_vres:
        matched = extend_by_VRE(matched, config=config,
                                base_year=config['opsd_vres_base_year'])

    if subsume_uncommon_fueltypes:
        matched = set_uncommon_fueltypes_to_other(matched)
    return matched.pipe(set_column_name, 'Matched Data')


///////////core.py/////////////////////////

#!/usr/bin/env python3



import logging
from os.path import join, expanduser, dirname, exists, isdir, abspath
from os import environ, makedirs

_writable_dir = join(expanduser('~'), '.local', 'share')
_data_dir = join(environ.get("XDG_DATA_HOME",
                             environ.get("APPDATA", _writable_dir)),
                 'powerplantmatching')

# data file configuration
package_config = {'custom_config': join(expanduser('~'),
                                        '.powerplantmatching_config.yaml'),
                  'data_dir': _data_dir,
                  'repo_data_dir': join(dirname(__file__), 'package_data'),
                  'downloaders': {}}

makedirs(join(package_config['data_dir'], 'data', 'in'), exist_ok=True)
makedirs(join(package_config['data_dir'], 'data', 'out'), exist_ok=True)


def _package_data(fn):
    return join(package_config['repo_data_dir'], fn)


def _data_in(fn):
    return join(package_config['data_dir'], 'data', 'in', fn)


def _data_out(fn, config=None):
    if config is None:
        return join(package_config['data_dir'], 'data', 'out', 'default', fn)
    else:
        return join(package_config['data_dir'], 'data', 'out', config['hash'],
                    fn)


del _data_dir
del _writable_dir


if not exists(_data_in('.')):
    makedirs(_data_in('.'))

# Logging: General Settings
logger = logging.getLogger(__name__)
logging.basicConfig(level=20)
logger.setLevel('INFO')
# Logging: File
logFormatter = logging.Formatter("%(asctime)s [%(threadName)-12.12s] "
                                 "[%(levelname)-5.5s]  %(message)s")
fileHandler = logging.FileHandler(join(package_config['data_dir'], 'PPM.log'))
fileHandler.setFormatter(logFormatter)
logger.addHandler(fileHandler)
# logger.info('Initialization complete.')

del logFormatter
del fileHandler


def get_config(filename=None, **overrides):
    
   # Import the configuration setting from yaml file.
   
    filename : str, optional
       
    DESCRIPTION. The default is None.
      overrides : dict
        DESCRIPTION.
   
    config : dict
        The configuration dictionary
    
    from hashlib import sha1
    from base64 import encodestring
    from six.moves import cPickle
    import yaml
    from logging import info

    if filename is None:
        custom_config = package_config['custom_config']
        if exists(custom_config):
            filename = custom_config
        else:
            filename = _package_data('config.yaml')

    with open(filename) as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
        config.update(overrides)

        sha1digest = sha1(cPickle.dumps(overrides)).digest()
        if len(dict(**overrides)) == 0:
            config['hash'] = 'default'
        else:
            config['hash'] = encodestring(sha1digest).decode('ascii')[2:12]

    if not isdir(_data_out('.', config=config)):
        makedirs(abspath(_data_out('.', config=config)))
        makedirs(abspath(_data_out('matches', config=config)))
        makedirs(abspath(_data_out('aggregations', config=config)))
        info('Outputs for this configuration will be saved under {}'
             .format(abspath(_data_out('.', config=config))))
        with open(_data_out('config.yaml', config=config), 'w') as file:
            yaml.dump(config, file, default_flow_style=False)
    return config


def get_obj_if_Acc(obj):
    from .accessor import PowerPlantAccessor
    if isinstance(obj, PowerPlantAccessor):
        return obj._obj
    else:
        return obj
    
    ////////////////////////data.py///////////////////
    
    # -*- coding: utf-8 -*-
# Copyright 2016-2020 Fabian Hofmann (FIAS), Jonas Hoersch (KIT, IAI) and
# Fabian Gotzens (FZJ, IEK-STE)

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 3 of the
# License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.


   Collection of power plant data bases and statistical data


from __future__ import print_function, absolute_import

import numpy as np
import pandas as pd
import requests
import xml.etree.ElementTree as ET
import re
import pycountry
import logging
import entsoe as entsoe_api

from .core import get_config, _package_data, _data_in, package_config
from .utils import (parse_if_not_stored, fill_geoposition, correct_manually,
                    config_filter, set_column_name)
from .heuristics import scale_to_net_capacities
from .cleaning import (gather_fueltype_info, gather_set_info,
                       gather_technology_info, clean_powerplantname,
                       clean_technology)

logger = logging.getLogger(__name__)
cget = pycountry.countries.get
net_caps = get_config()['display_net_caps']


def OPSD(rawEU=False, rawDE=False, rawDE_withBlocks=False, update=False,
         statusDE=['operating', 'reserve', 'special_case',
                   'shutdown_temporary'], config=None):
    
    Importer for the OPSD (Open Power Systems Data) database.
   
    rawEU : Boolean, default False
        Whether to return the raw EU (=non-DE) database.
    rawDE : Boolean, default False
        Whether to return the raw DE database.
    statusDE : list, default ['operating', 'reserve', 'special_case']
        Filter DE entries by operational status ['operating', 'shutdown',
        'reserve', etc.]
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
  
    config = get_config() if config is None else config

    opsd_DE = parse_if_not_stored('OPSD_DE', update, config, na_values=' ')
    opsd_EU = parse_if_not_stored('OPSD_EU', update, config, na_values=' ')
    if rawEU and rawDE:
        raise(NotImplementedError('''
                It is not possible to show both DE and EU raw databases at the
                same time as they have different formats. Choose only one!
                '''))
    if rawEU:
        return opsd_EU
    if rawDE:
        return opsd_DE
    if rawDE_withBlocks:
        DE_blocks = (opsd_DE
                     .loc[lambda x: ~(x['block_bnetza'].isna())]
                     .loc[lambda x: x['block_bnetza'] != x['name_bnetza']]
                     .assign(block=lambda x: x.block_bnetza.str.strip())
                     .loc[lambda x: ~(x.block.isin(['-', 'entfällt']))]
                     .assign(len_block=lambda x: x.block.apply(len)))
        upd = (DE_blocks.loc[lambda x: (x.len_block <= 6)]
                        .loc[lambda x: (x.block.str.slice(0, 5) != 'Block')]
                        .assign(block=lambda x: 'Block ' + x['block']))
        DE_blocks.update(upd)
        DE_blocks = DE_blocks.assign(name_bnetza=lambda x:
                                     x['name_bnetza'].str.strip() + ' '
                                     + x['block'])
        opsd_DE.update(DE_blocks)
        return opsd_DE.drop('Unnamed: 0', axis=1).set_index('id')
    
    opsd_EU = (opsd_EU.rename(columns=str.title)
                     .rename(columns={'Lat': 'lat',
                                      'Lon': 'lon',
                                      'Energy_Source': 'Fueltype',
                                      'Commissioned': 'DateIn',
                                      'Eic_Code': 'EIC'})
                     .eval('DateRetrofit = DateIn')
                     .assign(projectID=lambda s: 'OEU'
                             + pd.Series(s.index.astype(str), s.index))
                     .reindex(columns=config['target_columns']))

    opsd_DE = (opsd_DE.rename(columns=str.title)
                      .rename(columns={'Lat': 'lat',
                                       'Lon': 'lon',
                                       'Fuel': 'Fueltype',
                                       'Type': 'Set',
                                       'Country_Code': 'Country',
                                       'Capacity_Net_Bnetza': 'Capacity',
                                       'Commissioned': 'DateIn',
                                       'Shutdown': 'DateOut',
                                       'Eic_Code_Plant': 'EIC',
                                       'Id': 'projectID'})
                      .assign(Name=lambda d:
                                  d.Name_Bnetza.fillna(d.Name_Uba),
                              Fueltype=lambda d:
                                  d.Fueltype.fillna(d.Energy_Source_Level_1),
                              DateRetrofit=lambda d:
                                  d.Retrofit.fillna(d.DateIn)))
    if statusDE is not None:
        opsd_DE = opsd_DE.loc[opsd_DE.Status.isin(statusDE)]
    opsd_DE = opsd_DE.reindex(columns=config['target_columns'])
    return (pd.concat([opsd_EU, opsd_DE], ignore_index=True)
            .replace(dict(Fueltype={'Biomass and biogas': 'Bioenergy',
                                    'Fossil fuels': np.nan,
                                    'Mixed fossil fuels': 'Other',
                                    'Natural gas': 'Natural Gas',
                                    'Non-renewable waste': 'Waste',
                                    'Other bioenergy and renewable waste':
                                        'Bioenergy',
                                    'Other or unspecified energy sources':
                                        'Other',
                                    'Other fossil fuels': 'Other',
                                    'Other fuels': 'Other'},
                          Set={'IPP': 'PP'}))
            .replace({'Country': {'UK': u'GB', '[ \t]+|[ \t]+$.': ''},
                      'Capacity': {0.: np.nan}}, regex=True)
            .dropna(subset=['Capacity'])
            .assign(Name=lambda df: df.Name.str.title().str.strip(),
                    Fueltype=lambda df: df.Fueltype.str.title().str.strip())
            .powerplant.convert_alpha2_to_country()
            .pipe(set_column_name, 'OPSD')
            # .pipe(correct_manually, 'OPSD', config=config)
            .pipe(config_filter, name='OPSD', config=config)
            .pipe(gather_set_info)
            .pipe(clean_technology))


def GEO(raw=False, config=None):
   
    Importer for the GEO database.
   
    raw : Boolean, default False
        Whether to return the original dataset
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
  
    config = get_config() if config is None else config

    countries = config['target_countries']
    rename_cols = {'GEO_Assigned_Identification_Number': 'projectID',
                   'Name': 'Name',
                   'Type': 'Fueltype',
                   'Type_of_Plant_rng1': 'Technology',
                   'Type_of_Fuel_rng1_Primary': 'FuelClassification1',
                   'Type_of_Fuel_rng2_Secondary': 'FuelClassification2',
                   'Country': 'Country',
                   'Design_Capacity_MWe_nbr': 'Capacity',
                   'Year_Project_Commissioned': 'DateIn',
                   'Year_rng1_yr1': 'DateRetrofit',
                   'Longitude_Start': 'lon',
                   'Latitude_Start': 'lat'}

    geo = parse_if_not_stored('GEO', config=config, low_memory=False)
    if raw:
        return geo
    geo = geo.rename(columns=rename_cols)

    units = parse_if_not_stored('GEO_units', config=config, low_memory=False)

    # map from units to plants
    units['DateIn'] = units.Date_Commissioned_dt.str[:4].astype(float)
    units['Effiency'] = units.Unit_Efficiency_Percent.str.replace('%', '')\
                             .astype(float) / 100
    units = units.groupby('GEO_Assigned_Identification_Number')\
                 .agg({'DateIn': [min, max], 'Effiency': 'mean'})

    _ = geo.projectID.map(units.DateIn['min'])
    geo['DateIn'] = (geo.DateIn.str[:4].apply(pd.to_numeric, errors='coerce')
                     .where(lambda x: x > 1900).fillna(_))

    _ = geo.projectID.map(units.DateIn['max'])
    geo['DateRetrofit'] = geo.DateRetrofit.astype(float).fillna(_)

    _ = units.Effiency['mean']
    geo['Effiency'] = geo.projectID.map(_)
    return (geo.assign(projectID=lambda s: 'GEO' + s.projectID.astype(str))
              .query("Country in @countries")
              .replace({col: {'Gas': 'Natural Gas'} for col in
                        {'Fueltype', 'FuelClassification1',
                         'FuelClassification2'}})
             .pipe(gather_fueltype_info, search_col=['FuelClassification1'])
             .pipe(gather_technology_info, search_col=['FuelClassification1'],
                   config=config)
             .pipe(gather_set_info)
             .pipe(set_column_name, 'GEO')
             .pipe(config_filter, name='GEO', config=config)
             .pipe(clean_powerplantname)
             .pipe(clean_technology, generalize_hydros=True)
             .pipe(scale_to_net_capacities,
                   (not config['GEO']['net_capacity']))
             .pipe(config_filter, name='GEO', config=config)
             # .pipe(correct_manually, 'GEO', config=config)
             )


def CARMA(raw=False, config=None):
   
    Importer for the Carma database.
    
    raw : Boolean, default False
        Whether to return the original dataset
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
  
    config = get_config() if config is None else config

#    carmadata = pd.read_csv(_datconfig['CARMA']['fn'], low_memory=False)
    carma = parse_if_not_stored('CARMA', config=config, low_memory=False)
    if raw:
        return carma

    return (carma
            .rename(columns={'Geoposition': 'Geoposition',
                             'cap': 'Capacity',
                             'city': 'location',
                             'country': 'Country',
                             'fuel1': 'Fueltype',
                             'lat': 'lat',
                             'lon': 'lon',
                             'plant': 'Name',
                             'plant.id': 'projectID'})
            .assign(projectID=lambda df: 'CARMA' + df.projectID.astype(str))
            .loc[lambda df: df.Country.isin(config['target_countries'])]
            .replace(dict(Fueltype={'COAL': 'Hard Coal',
                                    'WAT': 'Hydro',
                                    'FGAS': 'Natural Gas',
                                    'NUC': 'Nuclear',
                                    'FLIQ': 'Oil',
                                    'WIND': 'Wind',
                                    'EMIT': 'Other',
                                    'GEO': 'Geothermal',
                                    'WSTH': 'Waste',
                                    'SUN': 'Solar',
                                    'BLIQ': 'Bioenergy',
                                    'BGAS': 'Bioenergy',
                                    'BSOL': 'Bioenergy',
                                    'OTH': 'Other'}))
            .pipe(clean_powerplantname)
            .drop_duplicates()
            .pipe(set_column_name, 'CARMA')
            .pipe(config_filter, name='CARMA', config=config)
            .pipe(gather_technology_info, config=config)
            .pipe(gather_set_info)
            .pipe(clean_technology)
            .pipe(scale_to_net_capacities, not config['CARMA']['net_capacity'])
            # .pipe(correct_manually, 'CARMA', config=config)
            )


def JRC(raw=False, config=None, update=False):
   
    Importer for the JRC Hydro-power plants database 

    config = get_config() if config is None else config
    url = config['JRC']['url']
    default_url = get_config(_package_data('config.yaml'))['JRC']['url']

    err = IOError(f'The URL seems to be outdated, please copy the new url '
                  f'\n\n\t{default_url}\n\nin your custom config file '
                  f'\n\n\t{package_config["custom_config"]}\n\nunder tag "JRC" '
                  '-> "url"')

    def parse_func():
        import requests
        from zipfile import ZipFile
        from io import BytesIO
        parse = requests.get(url)
        if parse.ok:
            content = parse.content
        else:
            raise(err)
        file = ZipFile(BytesIO(content))
        key = 'jrc-hydro-power-plant-database.csv'
        if key in file.namelist():
            return pd.read_csv(file.open(key))
        else:
            raise(err)

    df = parse_if_not_stored('JRC', update, config, parse_func=parse_func)
    if raw:
        return df
    df = (df.rename(columns={'id': 'projectID',
                               'name': 'Name',
                               'installed_capacity_MW': 'Capacity',
                               'country_code': 'Country',
                               'type': 'Technology',
                               'dam_height_m': 'DamHeight_m',
                               'volume_Mm3': 'Volume_Mm3',
                               'storage_capacity_MWh': 'StorageCapacity_MWh'})
            .eval('Duration = StorageCapacity_MWh / Capacity')
            .replace(dict(Technology={'HDAM': 'Reservoir',
                                      'HPHS': 'Pumped Storage',
                                      'HROR': 'Run-Of-River'}))
            .drop(columns=['pypsa_id', 'GEO'])
            .assign(Set='Store', Fueltype='Hydro')
            .powerplant.convert_alpha2_to_country()
            .pipe(config_filter))
    # TODO: Temporary section to deal with duplicate identifiers in the JRC
    # input file. Can be removed again, once the duplicates have been removed
    # in a new release.
    mask = df.projectID.duplicated(keep=False)
    df.loc[mask, 'projectID'] += (df.groupby('projectID').cumcount()
                                    .replace({0: 'a', 1: 'b', 2: 'c', 3: 'd'}))
    return df


def IWPDCY(config=None):
    
    
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()

    config = get_config() if config is None else config

    return (pd.read_csv(config['IWPDCY']['fn'],
                        encoding='utf-8', index_col='id')
            .assign(File='IWPDCY.csv',
                    projectID=lambda df: 'IWPDCY' + df.index.astype(str))
            .dropna(subset=['Capacity'])
            .pipe(set_column_name, 'IWPDCY')
            .pipe(config_filter, name='IWPDY', config=config)
            .pipe(gather_set_info)
            .pipe(correct_manually, 'IWPDCY', config=config))


def Capacity_stats(raw=False, level=2, config=None, update=False,
                   source='entsoe SO&AF', year='2016'):
    
    
    year : int
        Year of the data (range usually 2013-2017)
        (defaults to 2016)
    source : str
        Which statistics source from
        {'entsoe SO&AF', 'entsoe Statistics', 'EUROSTAT', ...}
        (defaults to 'entsoe SO&AF')
    
    df : pd.DataFrame
         Capacity statistics per country and fuel-type
    
    if config is None:
        config = get_config()

    df = parse_if_not_stored('Capacity_stats', update, config, index_col=0)
    if raw:
        return df

    countries = config['target_countries']
    df = (df.query('source == @source & year == @year')
          .rename(columns={'technology': 'Fueltype'}).rename(columns=str.title)
          .powerplant.convert_alpha2_to_country()
          # .query('Country in @countries')
          .replace(dict(Fueltype={
              'Bioenergy and other renewable fuels': 'Bioenergy',
              'Bioenergy and renewable waste': 'Waste',
              'Coal derivatives': 'Hard Coal',
              'Differently categorized fossil fuels': 'Other',
              'Differently categorized renewable energy sources':
              'Other',
              'Hard coal': 'Hard Coal',
              'Mixed fossil fuels': 'Other',
              'Natural gas': 'Natural Gas',
              'Other or unspecified energy sources': 'Other',
              'Tide, wave, and ocean': 'Other'}))
          .loc[lambda df: df.Fueltype.isin(config['target_fueltypes'])]
          .pipe(set_column_name, source.title()))
    return df


def GPD(raw=False, filter_other_dbs=True, update=False, config=None):
    
    #Importer for the `Global Power Plant Database`.
   
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
    
    config = get_config() if config is None else config

    # if outdated have a look at
    # http://datasets.wri.org/dataset/globalpowerplantdatabase
    url = config['GPD']['url']

    def parse_func():
        import requests
        from zipfile import ZipFile
        from io import BytesIO
        parse = requests.get(url)
        if parse.ok:
            content = parse.content
        else:
            IOError(f'URL {url} seems to be outdated, please doulble the '
                          'address at http://datasets.wri.org/dataset/'
                          'globalpowerplantdatabase update the and update the '
                          'url in your custom config file '
                          '{package_config["custom_config"]}')
        return pd.read_csv(ZipFile(BytesIO(content))
                           .open('global_power_plant_database.csv'))

    df = parse_if_not_stored('GPD', update, config, parse_func, index_col=0)
    if raw:
        return df

    other_dbs = []
    if filter_other_dbs:
        other_dbs = ['GEODB', 'CARMA', 'Open Power System Data', 'ENTSOE']
    countries = config['target_countries']
    return (df.rename(columns=lambda x: x.title())
            .query("Country_Long in @countries &"
                   " Geolocation_Source not in @other_dbs")
            .drop(columns='Country')
            .rename(columns={'Gppd_Idnr': 'projectID',
                             'Country_Long': 'Country',
                             'Primary_Fuel': 'Fueltype',
                             'Latitude': 'lat',
                             'Longitude': 'lon',
                             'Capacity_Mw': 'Capacity',
                             # 'Source': 'File'
                             'Commissioning_Year': 'DateIn'})
            .replace(dict(Fueltype={'Coal': 'Hard Coal',
                                    'Biomass': 'Bioenergy',
                                    'Gas': 'Natural Gas',
                                    'Wave and Tidal': 'Hydro'}))
            .pipe(clean_powerplantname)
            .pipe(set_column_name, 'GPD')
            .pipe(config_filter, name='GPD', config=config)
            # .pipe(gather_technology_info, config=config)
            # .pipe(gather_set_info)
            # .pipe(correct_manually, 'GPD', config=config)
            )




def ESE(raw=False, update=False, config=None):
    
    Importer for the ESE database.
   
    raw : Boolean, default False
        Whether to return the original dataset
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
    
    config = get_config() if config is None else config
    df = parse_if_not_stored('ESE', update, config, error_bad_lines=False)
    if raw:
        return df

    target_countries = config['target_countries']
    return (df.rename(columns=str.strip)
            .rename(columns={'Title': 'Name',
                             'Technology Mid-Type': 'Technology',
                             'Longitude': 'lon',
                             'Latitude': 'lat',
                             'Technology Broad Category': 'Fueltype'})
            .assign(Set='Store',
                    projectID='ESE' + df.index.astype(str),
                    DateIn=lambda df: (df['Commissioned'].str[-4:]
                                       .apply(pd.to_numeric, errors='coerce')),
                    Capacity=df['Rated Power'] / 1e3)
            .query("Status == 'Operational' & Country in @target_countries")
            .pipe(clean_powerplantname)
            .pipe(clean_technology, generalize_hydros=True)
            .replace(dict(Fueltype={u'Electro-chemical': 'Battery',
                                    u'Pumped Hydro Storage': 'Hydro'}))
            .pipe(set_column_name, 'ESE')
            .pipe(config_filter, name='ESE', config=config)
            # .pipe(correct_manually, 'ESE', config=config)
            )


def ENTSOE(update=False, raw=False, entsoe_token=None, config=None):
    
    Importer for the list of installed generators provided by the ENTSO-E
    Trasparency Project. Geographical information is not given.
    If update=True, the dataset is parsed through a request to
   
    Internet connection requiered. If raw=True, the same request is done, but
    the unprocessed data is returned.
    Parameters
    ----------
    update : Boolean, Default False
        Whether to update the database through a request to the ENTSO-E
        transparency plattform
    raw : Boolean, Default False
        Whether to return the raw data, obtained from the request to
        the ENTSO-E transparency platform
    entsoe_token: String
        Security token of the ENTSO-E Transparency platform
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
    Note: For obtaining a security token refer to section 2 of the
    RESTful API documentation of the ENTSOE-E Transparency platform
   
    config = get_config() if config is None else config

    def parse_entsoe():
        assert entsoe_token is not None, "entsoe_token is missing"
        url = 'https://transparency.entsoe.eu/api'
        # retrieved from pd.read_html('https://transparency.entsoe.eu/content/stat
        # ic_content/Static%20content/web%20api/Guide.html#_request_methods')[-1]
        domains = list(entsoe_api.mappings.BIDDING_ZONES.values())

        level1 = ['registeredResource.name', 'registeredResource.mRID']
        level2 = ['voltage_PowerSystemResources.highVoltageLimit', 'psrType']
        level3 = ['quantity']

        def namespace(element):
            m = re.match('\{.*\}', element.tag)
            return m.group(0) if m else ''

        entsoe = pd.DataFrame()
        logger.info(f"Retrieving data from {url}")
        for domain in domains:
            ret = requests.get(url, params=dict(
                securityToken=entsoe_token, documentType='A71',
                processType='A33', In_Domain=domain,
                periodStart='201612312300', periodEnd='201712312300'))
            etree = ET.fromstring(ret.content)
            ns = namespace(etree)
            df_domain = pd.DataFrame(columns=level1+level2+level3+['Country'])
            for i, level in enumerate([level1, level2, level3]):
                for arg in level:
                    df_domain[arg] = [
                        e.text for e in etree.findall('*/' * (i+1) + ns + arg)]
            entsoe = entsoe.append(df_domain, ignore_index=True)
        return entsoe

    if config['entsoe_token'] is not None:
        entsoe_token = config['entsoe_token']
        df = parse_if_not_stored('ENTSOE', update, config, parse_entsoe)
    else:
        if update:
            logger.info('No entsoe_token in config.yaml given, '
                        'falling back to stored version.')
        df = parse_if_not_stored('ENTSOE', update, config)

    if raw:
        return df

    fuelmap = entsoe_api.mappings.PSRTYPE_MAPPINGS
    country_map_entsoe = pd.read_csv(_package_data('entsoe_country_codes.csv'),
                                     index_col=0).rename(index=str).Country
    countries = config['target_countries']

    return (df.rename(columns={'psrType': 'Fueltype',
                               'quantity': 'Capacity',
                               'registeredResource.mRID': 'projectID',
                               'registeredResource.name': 'Name'})
            .reindex(columns=config['target_columns'])
            .replace({'Fueltype': fuelmap})
            .drop_duplicates('projectID')
            .assign(EIC=lambda df: df.projectID,
                    Country=lambda df: df.projectID.str[:2]
                                         .map(country_map_entsoe),
                    Name=lambda df: df.Name.str.title(),
                    Fueltype=lambda df: df.Fueltype.replace(
                        {'Fossil Hard coal': 'Hard Coal',
                         'Fossil Coal-derived gas': 'Other',
                         '.*Hydro.*': 'Hydro',
                         '.*Oil.*': 'Oil',
                         '.*Peat': 'Bioenergy',
                         'Fossil Brown coal/Lignite': 'Lignite',
                         'Biomass': 'Bioenergy',
                         'Fossil Gas': 'Natural Gas',
                         'Marine': 'Other',
                         'Wind Offshore': 'Offshore',
                         'Wind Onshore': 'Onshore'}, regex=True),
                    Capacity=lambda df: pd.to_numeric(df.Capacity))
            .powerplant.convert_alpha2_to_country()
            .pipe(clean_powerplantname)
            # .query('Country in @countries')
            .pipe(fill_geoposition, use_saved_locations=True, saved_only=True)
            .query('Capacity > 0')
            .pipe(gather_technology_info, config=config)
            .pipe(gather_set_info)
            .pipe(clean_technology)
            .pipe(set_column_name, 'ENTSOE')
            .pipe(config_filter, name='ENTSOE', config=config)
            # .pipe(correct_manually, 'ENTSOE', config=config)
            )


# def OSM():

#   Parser and Importer for Open Street Map power plant data.
  
    import requests
    overpass_url = "http://overpass-api.de/api/interpreter"
    overpass_query = [out:json][timeout:210];
    area["name"="Luxembourg"]->.boundaryarea;
    (
#//query part for: power=plant 
    node["power"="plant"](area.boundaryarea);
    way["power"="plant"](area.boundaryarea);
   relation["power"="plant"](area.boundaryarea);
    node["power"="generator"](area.boundaryarea);
    way["power"="generator"](area.boundaryarea);
    relation["power"="generator"](area.boundaryarea);
   );
    out body;
    
    response = requests.get(overpass_url,
                            params={'data': overpass_query})
    data = response.json()
    df = pd.DataFrame(data['elements'])
    df = pd.concat([df.drop(columns='tags'), df.tags.apply(pd.Series)], axis=1)


def WEPP(raw=False, config=None):
   
    
    raw : Boolean, default False
        Whether to return the original dataset
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
   
    config = get_config() if config is None else config

    # Define the appropriate datatype for each column (some columns e.g.
    # 'YEAR' cannot be integers, as there are N/A values, which np.int
    # does not yet(?) support.)
    datatypes = {'UNIT': str, 'PLANT': str, 'COMPANY': str, 'MW': np.float64,
                 'STATUS': str, 'YEAR': np.float64, 'UTYPE': str, 'FUEL': str,
                 'FUELTYPE': str, 'ALTFUEL': str, 'SSSMFR': str,
                 'BOILTYPE': str, 'TURBMFR': str, 'TURBTYPE': str,
                 'GENMFR': str, 'GENTYPE': str, 'SFLOW': np.float64,
                 'SPRESS': np.float64, 'STYPE': str, 'STEMP': np.float64,
                 'REHEAT1': np.float64, 'REHEAT2': np.float64, 'PARTCTL': str,
                 'PARTMFR': str, 'SO2CTL': str, 'FGDMFR': str, 'NOXCTL': str,
                 'NOXMFR': str, 'AE': str, 'CONstr, UCT': str, 'COOL': str,
                 'RETIRE': np.float64, 'CITY': str, 'STATE': str,
                 'COUNTRY': str, 'AREA': str, 'SUBREGION': str,
                 'POSTCODE': str, 'PARENT': str, 'ELECTYPE': str,
                 'BUSTYPE': str, 'COMPID': str, 'LOCATIONID': str,
                 'UNITID': str}
    # Now read the Platts WEPP Database
    wepp = pd.read_csv(config['WEPP']['source_file'], dtype=datatypes,
                       encoding='utf-8')
    if raw:
        return wepp

    # Fit WEPP-column names to our specifications
    wepp.columns = wepp.columns.str.title()
    wepp.rename(columns={'Unit': 'Name',
                         'Fuel': 'Fueltype',
                         'Fueltype': 'Technology',
                         'Mw': 'Capacity',
                         'Year': 'DateIn',
                         'Retire': 'DateOut',
                         'Lat': 'lat',
                         'Lon': 'lon',
                         'Unitid': 'projectID'}, inplace=True)
    wepp.loc[:, 'DateRetrofit'] = wepp.DateIn
    # Do country transformations and drop those which are not in definded scope
    c = {'ENGLAND & WALES': u'UNITED KINGDOM',
         'GIBRALTAR': u'SPAIN',
         'SCOTLAND': u'UNITED KINGDOM'}
    wepp.Country = wepp.Country.replace(c).str.title()
    wepp = (wepp.loc[lambda df: df.Country.isin(config['target_countries'])]
                .loc[lambda df: df.Status.isin(['OPR', 'CON'])]
                .assign(File=config['WEPP']['source_file']))
    # Replace fueltypes
    d = {'AGAS': 'Bioenergy',    # Syngas from gasified agricultural waste
         'BFG': 'Other',         # blast furnance gas -> "Hochofengas"
         'BGAS': 'Bioenergy',
         'BIOMASS': 'Bioenergy',
         'BL': 'Bioenergy',
         'CGAS': 'Hard Coal',
         'COAL': 'Hard Coal',
         'COG': 'Other',         # coke oven gas -> deutsch: "Hochofengas"
         'COKE': 'Hard Coal',
         'CSGAS': 'Hard Coal',   # Coal-seam-gas
         'CWM': 'Hard Coal',     # Coal-water mixture (aka coal-water slurry)
         'DGAS': 'Other',        # sewage digester gas -> deutsch: "Klaergas"
         'FGAS': 'Other',        # Flare gas or wellhead gas or associated gas
         'GAS': 'Natural Gas',
         'GEO': 'Geothermal',
         'H2': 'Other',          # Hydrogen gas
         'HZDWST': 'Waste',      # Hazardous waste
         'INDWST': 'Waste',      # Industrial waste or refinery waste
         'JET': 'Oil',           # Jet fuels
         'KERO': 'Oil',          # Kerosene
         'LGAS': 'Other',        # landfill gas -> deutsch: "Deponiegas"
         'LIGNIN': 'Bioenergy',
         'LIQ': 'Other',         # (black) liqour -> deutsch: "Schwarzlauge",
                                 #    die bei Papierherstellung anfaellt
         'LNG': 'Natural Gas',   # Liquified natural gas
         'LPG': 'Natural Gas',   # Liquified petroleum gas (u. butane/propane)
         'MBM': 'Bioenergy',     # Meat and bonemeal
         'MEDWST': 'Bioenergy',  # Medical waste
         'MGAS': 'Other',        # mine gas -> deutsch: "Grubengas"
         'NAP': 'Oil',           # naphta
         'OGAS': 'Oil',          # Gasified crude oil/refinery bottoms/bitumen
         'PEAT': 'Other',
         'REF': 'Waste',
         'REFGAS': 'Other',      # Syngas from gasified refuse
         'RPF': 'Waste',         # Waste paper and/or waste plastic
         'PWST': 'Other',        # paper mill waste
         'RGAS': 'Other',        # refinery off-gas -> deutsch: "Raffineriegas"
         'SHALE': 'Oil',
         'SUN': 'Solar',
         'TGAS': 'Other',        # top gas -> deutsch: "Hochofengas"
         'TIRES': 'Other',       # Scrap tires
         'UNK': 'Other',
         'UR': 'Nuclear',
         'WAT': 'Hydro',
         'WOOD': 'Bioenergy',
         'WOODGAS': 'Bioenergy',
         'WSTGAS': 'Other',      # waste gas -> deutsch: "Industrieabgas"
         'WSTWSL': 'Waste',      # Wastewater sludge
         'WSTH': 'Waste'}
    wepp.Fueltype = wepp.Fueltype.replace(d)
    # Fill NaNs to allow str actions
    wepp.Technology.fillna('', inplace=True)
    wepp.Turbtype.fillna('', inplace=True)
    # Correct technology infos:
    wepp.loc[wepp.Technology.str.contains('LIG', case=False),
             'Fueltype'] = 'Lignite'
    wepp.loc[wepp.Turbtype.str.contains('KAPLAN|BULB', case=False),
             'Technology'] = 'Run-Of-River'
    wepp.Technology = wepp.Technology.replace({'CONV/PS': 'Pumped Storage',
                                               'CONV': 'Reservoir',
                                               'PS': 'Pumped Storage'})
    tech_st_pattern = ['ANTH', 'BINARY', 'BIT', 'BIT/ANTH', 'BIT/LIG',
                       'BIT/SUB', 'BIT/SUB/LIG', 'COL', 'DRY ST', 'HFO', 'LIG',
                       'LIG/BIT', 'PWR', 'RDF', 'SUB']
    tech_ocgt_pattern = ['AGWST', 'LITTER', 'RESID', 'RICE', 'STRAW']
    tech_ccgt_pattern = ['LFO']
    wepp.loc[wepp.Technology.isin(tech_st_pattern),
             'Technology'] = 'Steam Turbine'
    wepp.loc[wepp.Technology.isin(tech_ocgt_pattern), 'Technology'] = 'OCGT'
    wepp.loc[wepp.Technology.isin(tech_ccgt_pattern), 'Technology'] = 'CCGT'
    ut_ccgt_pattern = ['CC', 'GT/C', 'GT/CP', 'GT/CS', 'GT/ST', 'ST/C',
                       'ST/CC/GT', 'ST/CD', 'ST/CP', 'ST/CS', 'ST/GT',
                       'ST/GT/IC', 'ST/T', 'IC/CD', 'IC/CP', 'IC/GT']
    ut_ocgt_pattern = ['GT', 'GT/D', 'GT/H', 'GT/HY', 'GT/IC', 'GT/S', 'GT/T',
                       'GTC']
    ut_st_pattern = ['ST', 'ST/D']
    ut_ic_pattern = ['IC', 'IC/H']
    wepp.loc[wepp.Utype.isin(ut_ccgt_pattern), 'Technology'] = 'CCGT'
    wepp.loc[wepp.Utype.isin(ut_ocgt_pattern), 'Technology'] = 'OCGT'
    wepp.loc[wepp.Utype.isin(ut_st_pattern), 'Technology'] = 'Steam Turbine'
    wepp.loc[wepp.Utype.isin(ut_ic_pattern),
             'Technology'] = 'Combustion Engine'
    wepp.loc[wepp.Utype == 'WTG', 'Technology'] = 'Onshore'
    wepp.loc[wepp.Utype == 'WTG/O', 'Technology'] = 'Offshore'
    wepp.loc[(wepp.Fueltype == 'Solar') & (wepp.Utype.isin(ut_st_pattern)),
             'Technology'] = 'CSP'
    # Derive the SET column
    chp_pattern = ['CC/S', 'CC/CP', 'CCSS/P', 'GT/CP', 'GT/CS', 'GT/S', 'GT/H',
                   'IC/CP', 'IC/H', 'ST/S', 'ST/H', 'ST/CP', 'ST/CS', 'ST/D']
    wepp.loc[wepp.Utype.isin(chp_pattern), 'Set'] = 'CHP'
    wepp.loc[wepp.Set.isnull(), 'Set'] = 'PP'
    # Clean up the mess
    wepp.Fueltype = wepp.Fueltype.str.title()
    wepp.loc[wepp.Technology.str.len() > 4, 'Technology'] = \
        wepp.loc[wepp.Technology.str.len() > 4, 'Technology'].str.title()
    # Done!
    wepp.datasetID = 'WEPP'
    return (wepp
            .pipe(set_column_name, 'WEPP')
            .pipe(config_filter, name='WEPP', config=config)
            .pipe(scale_to_net_capacities,
                  (not config['WEPP']['net_capacity']))
            .pipe(correct_manually, 'WEPP', config=config))


def UBA(header=9, skipfooter=26, prune_wind=True, prune_solar=True,
        config=None, update=False, raw=False):
    
    header : int, Default 9
        The zero-indexed row in which the column headings are found.
    skipfooter : int, Default 26
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
 
    config = get_config() if config is None else config

    parse_func = lambda url: pd.read_excel(url, skipfooter=skipfooter,
                                           na_values='n.b.', header=header)
    uba = parse_if_not_stored('UBA', update, config, parse_func)
    if raw:
        return uba
    uba = uba.rename(columns={
        u'Kraftwerksname / Standort': 'Name',
        u'Elektrische Bruttoleistung (MW)': 'Capacity',
        u'Inbetriebnahme  (ggf. Ertüchtigung)': 'DateIn',
        u'Primärenergieträger': 'Fueltype',
        u'Anlagenart': 'Technology',
        u'Fernwärme-leistung (MW)': 'CHP',
        u'Standort-PLZ': 'PLZ'})
    from .heuristics import PLZ_to_LatLon_map
    uba = (uba.assign(
        Name=uba.Name.replace({'\s\s+': ' '}, regex=True),
        lon=uba.PLZ.map(PLZ_to_LatLon_map()['lon']),
        lat=uba.PLZ.map(PLZ_to_LatLon_map()['lat']),
        DateIn=uba.DateIn.str.replace(
            "\(|\)|\/|\-", " ").str.split(' ').str[0].astype(float),
        Country='Germany',
        File='kraftwerke-de-ab-100-mw.xls',
        projectID=['UBA{:03d}'.format(i + header + 2) for i in uba.index],
        Technology=uba.Technology.replace({
            u'DKW': 'Steam Turbine',
            u'DWR': 'Pressurized Water Reactor',
            u'G/AK': 'Steam Turbine',
            u'GT': 'OCGT',
            u'GuD': 'CCGT',
            u'GuD / HKW': 'CCGT',
            u'HKW': 'Steam Turbine',
            u'HKW (DT)': 'Steam Turbine',
            u'HKW / GuD': 'CCGT',
            u'HKW / SSA': 'Steam Turbine',
            u'IKW': 'OCGT',
            u'IKW / GuD': 'CCGT',
            u'IKW / HKW': 'Steam Turbine',
            u'IKW / HKW / GuD': 'CCGT',
            u'IKW / SSA': 'OCGT',
            u'IKW /GuD': 'CCGT',
            u'LWK': 'Run-Of-River',
            u'PSW': 'Pumped Storage',
            u'SWK': 'Reservoir Storage',
            u'SWR': 'Boiled Water Reactor'})))
    uba.loc[uba.CHP.notnull(), 'Set'] = 'CHP'
    uba = uba.pipe(gather_set_info)
    uba.loc[uba.Fueltype == 'Wind (O)', 'Technology'] = 'Offshore'
    uba.loc[uba.Fueltype == 'Wind (L)', 'Technology'] = 'Onshore'
    uba.loc[uba.Fueltype.str.contains('Wind'), 'Fueltype'] = 'Wind'
    uba.loc[uba.Fueltype.str.contains('Braunkohle'), 'Fueltype'] = 'Lignite'
    uba.loc[uba.Fueltype.str.contains('Steinkohle'), 'Fueltype'] = 'Hard Coal'
    uba.loc[uba.Fueltype.str.contains('Erdgas'), 'Fueltype'] = 'Natural Gas'
    uba.loc[uba.Fueltype.str.contains('HEL'), 'Fueltype'] = 'Oil'
    uba.Fueltype = uba.Fueltype.replace({u'Biomasse': 'Bioenergy',
                                         u'Gichtgas': 'Other',
                                         u'HS': 'Oil',
                                         u'Konvertergas': 'Other',
                                         u'Licht': 'Solar',
                                         u'Raffineriegas': 'Other',
                                         u'Uran': 'Nuclear',
                                         u'Wasser': 'Hydro',
                                         u'\xd6lr\xfcckstand': 'Oil'})
    uba.Name.replace([r'(?i)oe', r'(?i)ue'], [u'ö', u'ü'], regex=True,
                     inplace=True)
    if prune_wind:
        uba = uba.loc[lambda x: x.Fueltype != 'Wind']
    if prune_solar:
        uba = uba.loc[lambda x: x.Fueltype != 'Solar']
    return (uba
            .pipe(set_column_name, 'UBA')
            .pipe(scale_to_net_capacities, not config['UBA']['net_capacity'])
            # .pipe(config_filter, name='UBA', config=config)
            # .pipe(correct_manually, 'UBA', config=config)
            )


def BNETZA(header=9, sheet_name='Gesamtkraftwerksliste BNetzA',
           prune_wind=True, prune_solar=True, raw=False, update=False,
           config=None):
 
    #//Importer for the database put together by Germany's 'Federal Network Agency' (dt. 'Bundesnetzagentur' (BNetzA)).

   

    header : int, Default 9
        The zero-indexed row in which the column headings are found.
    raw : Boolean, default False
        Whether to return the original dataset
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
   
    config = get_config() if config is None else config

    parse_func = lambda url: pd.read_excel(url, header=header,
                                           sheet_name=sheet_name,
                                           parse_dates=False)
    bnetza = parse_if_not_stored('BNETZA', update, config, parse_func)

    if raw:
        return bnetza
    bnetza = bnetza.rename(columns={
        'Kraftwerksnummer Bundesnetzagentur': 'projectID',
        'Kraftwerksname': 'Name',
        'Netto-Nennleistung (elektrische Wirkleistung) in MW': 'Capacity',
        'Wärmeauskopplung (KWK)\n(ja/nein)': 'Set',
        'Ort\n(Standort Kraftwerk)': 'Ort',
        ('Auswertung\nEnergieträger (Zuordnung zu einem '
         'Hauptenergieträger bei Mehreren Energieträgern)'): 'Fueltype',
        'Kraftwerksstatus \n(in Betrieb/\nvorläufig '
        'stillgelegt/\nsaisonale Konservierung\nNetzreserve/ '
        'Sicherheitsbereitschaft/\nSonderfall)': 'Status',
         ('Aufnahme der kommerziellen Stromerzeugung der derzeit '
          'in Betrieb befindlichen Erzeugungseinheit\n(Datum/Jahr)'):
         'DateIn',
         'PLZ\n(Standort Kraftwerk)': 'PLZ'})
    # If BNetzA-Name is empty replace by company, if this is empty by city.

    from .heuristics import PLZ_to_LatLon_map

    pattern = '|'.join(['.*(?i)betrieb', '.*(?i)gehindert', '(?i)vorläufig.*',
                        'Sicherheitsbereitschaft', 'Sonderfall'])
    bnetza = (bnetza.assign(
              lon=bnetza.PLZ.map(PLZ_to_LatLon_map()['lon']),
              lat=bnetza.PLZ.map(PLZ_to_LatLon_map()['lat']),
              Name=bnetza.Name.where(bnetza.Name.str.len().fillna(0) > 4,
                                     bnetza.Unternehmen + ' '
                                     + bnetza.Name.fillna(''))
                              .fillna(bnetza.Ort).str.strip(),
              DateIn=bnetza.DateIn.str[:4]
                               .apply(pd.to_numeric, errors='coerce'),
              Blockname=bnetza.Blockname.replace(
                  {'.*(GT|gasturbine).*': 'OCGT',
                   '.*(DT|HKW|(?i)dampfturbine|(?i)heizkraftwerk).*':
                       'Steam Turbine',
                   '.*GuD.*': 'CCGT'}, regex=True))
              [lambda df: df.projectID.notna()
               & df.Status.str.contains(pattern, regex=True, case=False)]
              .pipe(gather_technology_info,
                    search_col=['Name', 'Fueltype', 'Blockname'],
                    config=config))

    add_location_b = (bnetza[bnetza.Ort.notnull()]
                      .apply(lambda ds: (ds['Ort'] not in ds['Name'])
                             and (str.title(ds['Ort']) not in ds['Name']),
                             axis=1))
    bnetza.loc[bnetza.Ort.notnull() & add_location_b, 'Name'] = (
        bnetza.loc[bnetza.Ort.notnull() & add_location_b, 'Ort']
        + ' '
        + bnetza.loc[bnetza.Ort.notnull() & add_location_b, 'Name'])

    techmap = {'solare': 'PV',
               'Laufwasser': 'Run-Of-River',
               'Speicherwasser': 'Reservoir',
               'Pumpspeicher': 'Pumped Storage'}
    for fuel in techmap:
        bnetza.loc[bnetza.Fueltype.str.contains(fuel, case=False),
                   'Technology'] = techmap[fuel]
    # Fueltypes
    bnetza.Fueltype.replace({'Erdgas': 'Natural Gas',
                             'Steinkohle': 'Hard Coal',
                             'Braunkohle': 'Lignite',
                             'Wind.*': 'Wind',
                             'Solar.*': 'Solar',
                             '.*(?i)energietr.*ger.*\n.*': 'Other',
                             'Kern.*': 'Nuclear',
                             'Mineral.l.*': 'Oil',
                             'Biom.*': 'Bioenergy',
                             '.*(?i)(e|r|n)gas': 'Other',
                             'Geoth.*': 'Geothermal',
                             'Abfall': 'Waste',
                             '.*wasser.*': 'Hydro',
                             '.*solar.*': 'PV'},
                            regex=True, inplace=True)
    if prune_wind:
        bnetza = bnetza[lambda x: x.Fueltype != 'Wind']
    if prune_solar:
        bnetza = bnetza[lambda x: x.Fueltype != 'Solar']
    # Filter by country
    bnetza = bnetza[~bnetza.Bundesland.isin([u'Österreich', 'Schweiz',
                                             'Luxemburg'])]
    return (bnetza.assign(Country='Germany',
                          Set=bnetza.Set.fillna('Nein').str.title()
                              .replace({'Ja': 'CHP', 'Nein': 'PP'}))
            .pipe(set_column_name, 'BNETZA')
            # .pipe(config_filter, name='BNETZA', config=config)
            # .pipe(correct_manually, 'BNETZA', config=config)
            )


def OPSD_VRE(config=None, raw=False):
   
 #//Importer for the OPSD (Open Power Systems Data) renewables (VRE) database. This sqlite database is very big and hence not part of the package.
    
    
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
    
    config = get_config() if config is None else config

    df = parse_if_not_stored('OPSD_VRE', index_col=0, low_memory=False)
    if raw:
        return df

    return df.rename(columns={'energy_source_level_2': 'Fueltype',
                              'technology': 'Technology',
                              'data_source': 'file',
                              'country': 'Country',
                              'electrical_capacity': 'Capacity',
                              'municipality': 'Name'})\
            .assign(DateIn=lambda df:
                    df.commissioning_date.str[:4].astype(float))\
            .powerplant.convert_alpha2_to_country()\
            .pipe(set_column_name, 'OPSD_VRE')\
            .pipe(config_filter, config=config)\
            .drop('Name', axis=1)


def OPSD_VRE_country(country, config=None, raw=False):
   
   # Get country specifig data from OPSD for renewables, if available.
    Available for DE, FR, PL, CH, DK, CZ and SE (last update: 09/2020).
    
    config = get_config() if config is None else config

    df = parse_if_not_stored(f'OPSD_VRE_{country}', low_memory=False)
    if raw:
        return df

    return (df.assign(Country=country)
              .rename(columns={'energy_source_level_2': 'Fueltype',
                               'technology': 'Technology',
                               'data_source': 'file',
                               'country': 'Country',
                               'electrical_capacity': 'Capacity',
                               'municipality': 'Name'})
              .powerplant.convert_alpha2_to_country()
              .pipe(config_filter, config=config)
              .drop('Name', axis=1))


def IRENA_stats(config=None):
    
    config : dict, default None
        Add custom specific configuration,
        e.g. powerplantmatching.config.get_config(target_countries='Italy'),
        defaults to powerplantmatching.config.get_config()
    
    if config is None:
        config = get_config()

    # Read the raw dataset
    df = pd.read_csv(_data_in('IRENA_CapacityStatistics2017.csv'),
                     encoding='utf-8')
    # "Unpivot"
    df = pd.melt(df, id_vars=['Indicator', 'Technology', 'Country'],
                 var_name='Year',
                 value_vars=[str(i) for i in range(2000, 2017, 1)],
                 value_name='Capacity')
    # Drop empty
    df.dropna(axis=0, subset=['Capacity'], inplace=True)
    # Drop generations
    df = df[df.Indicator == 'Electricity capacity (MW)']
    df.drop('Indicator', axis=1, inplace=True)
    # Drop countries out of scope
    df.Country.replace({'Czechia': u'Czech Republic',
                        'UK': u'United Kingdom'}, inplace=True)
    df = df.loc[lambda df: df.Country.isin(config['target_countries'])]
    # Convert to numeric
    df.Year = df.Year.astype(int)
    df.Capacity = df.Capacity.str.strip().str.replace(' ', '').astype(float)
    # Handle Fueltypes and Technologies
    d = {u'Bagasse': 'Bioenergy',
         u'Biogas': 'Bioenergy',
         u'Concentrated solar power': 'Solar',
         u'Geothermal': 'Geothermal',
         u'Hydro 1-10 MW': 'Hydro',
         u'Hydro 10+ MW': 'Hydro',
         u'Hydro <1 MW': 'Hydro',
         u'Liquid biofuels': 'Bioenergy',
         u'Marine': 'Hydro',
         u'Mixed and pumped storage': 'Hydro',
         u'Offshore wind energy': 'Wind',
         u'Onshore wind energy': 'Wind',
         u'Other solid biofuels': 'Bioenergy',
         u'Renewable municipal waste': 'Waste',
         u'Solar photovoltaic': 'Solar'}
    df.loc[:, 'Fueltype'] = df.Technology.map(d)
#    df = df.loc[lambda df: df.Fueltype.isin(config['target_fueltypes'])]
    d = {u'Concentrated solar power': 'CSP',
         u'Solar photovoltaic': 'PV',
         u'Onshore wind energy': 'Onshore',
         u'Offshore wind energy': 'Offshore'}
    df.Technology.replace(d, inplace=True)
    df.loc[:, 'Set'] = 'PP'
    return df.reset_index(drop=True).pipe(set_column_name, 'IRENA Statistics')


///////////////////////////duke.py////////////////////

from __future__ import absolute_import, print_function
import logging
import os
import subprocess as sub
import shutil
import tempfile
import pandas as pd
import numpy as np
from .core import _package_data
logger = logging.getLogger(__name__)


def add_geoposition_for_duke(df):
    
   # Returns the same pandas.Dataframe with an additional column "Geoposition"
   # which concats the latitude and longitude of the powerplant in a string
   
    if not df.loc[:, ['lat', 'lon']].isnull().all().all():
        return df.assign(Geoposition=df[['lat', 'lon']].astype(str)
                         .apply(lambda s: ','.join(s), axis=1)
                         .replace('nan,nan', np.nan))
    else:
        return df.assign(Geoposition=np.nan)


def duke(datasets, labels=['one', 'two'], singlematch=False,
         showmatches=False, keepfiles=False, showoutput=False):
   
   # Run duke in different modes (Deduplication or Record Linkage Mode) to
    either locate duplicates in one database or find the similar entries in two
    different datasets. In RecordLinkagesMode (match two databases) please
    
    set singlematch=True and use best_matches() afterwards
    Parameters
    ----------
    datasets : pd.DataFrame or [pd.DataFrame]
        A single dataframe is run in deduplication mode, while multiple ones
        are linked
    labels : [str], default ['one', 'two']
        Labels for the linked dataframe
    singlematch: boolean, default False
        Only in Record Linkage Mode. Only report the best match for each entry
        of the first named dataset. This does not guarantee a unique match in
        the second named dataset.
    keepfiles : boolean, default False
        If true, do not delete temporary files
    
    dedup = isinstance(datasets, pd.DataFrame)
    if dedup:
        # Deduplication mode
        duke_config = "Deleteduplicates.xml"
        datasets = [datasets]
    else:
        duke_config = "Comparison.xml"

    duke_bin_dir = _package_data('duke_binaries')

    os.environ['CLASSPATH'] = \
        os.pathsep.join([os.path.join(duke_bin_dir, r)
                        for r in os.listdir(duke_bin_dir)])
    tmpdir = tempfile.mkdtemp()

    try:
        shutil.copyfile(os.path.join(_package_data(duke_config)),
                        os.path.join(tmpdir, "config.xml"))

        logger.debug("Comparing files: %s", ", ".join(labels))

        for n, df in enumerate(datasets):
            df = add_geoposition_for_duke(df)
#            due to index unity (see https://github.com/larsga/Duke/issues/236)
            if n == 1:
                shift_by = (datasets[0].index.max()+1)
                df.index += shift_by
            df.to_csv(os.path.join(tmpdir, "file{}.csv".format(n+1)),
                      index_label='id')
            if n == 1:
                df.index -= shift_by

        args = ['java', '-Dfile.encoding=UTF-8', 'no.priv.garshol.duke.Duke',
                '--linkfile=linkfile.txt']
        if singlematch:
            args.append('--singlematch')
        if showmatches:
            args.append('--showmatches')
            stdout = sub.PIPE
        else:
            stdout = None
        args.append('config.xml')

        try:        
            run = sub.Popen(args, stderr=sub.PIPE, cwd=tmpdir, stdout=stdout,
                            universal_newlines=True)
        except FileNotFoundError:
            err = "Java was not found on your system."
            logger.error(err)
            raise FileNotFoundError(err)

        _, stderr = run.communicate()

        if showmatches:
            print(_)

        logger.debug("Stderr: {}".format(stderr))
        if any(word in stderr.lower() for word in ['error', 'fehler']):
            raise RuntimeError("duke failed: {}".format(stderr))

        if dedup:
            return pd.read_csv(os.path.join(tmpdir, 'linkfile.txt'),
                               encoding='utf-8', usecols=[1, 2], names=labels)
        else:
            res = pd.read_csv(os.path.join(tmpdir, 'linkfile.txt'),
                              usecols=[1, 2, 3], names=labels + ['scores'])
            res.iloc[:, 1] -= shift_by
            return res

    finally:
        if keepfiles:
            logger.debug('Files of the duke run are kept in {}'.format(tmpdir))
        else:
            shutil.rmtree(tmpdir)
            logger.debug('Files of the duke run have been deleted in {}'
                         .format(tmpdir))
            
//////////////////////////export.py////////////////

 from .core import _data_out, get_obj_if_Acc
from .heuristics import set_denmark_region_id, set_known_retire_years

import pandas as pd
import numpy as np
import pycountry
import logging
logger = logging.getLogger(__name__)
cget = pycountry.countries.get


def to_pypsa_names(df):
    
    #Rename the columns of the powerplant data according to the
    convention in PyPSA.
    
    Arguments:
        df {pandas.DataFrame} -- powerplant data
    Returns:
        pandas.DataFrame -- Column renamed dataframe
    
    df = get_obj_if_Acc(df)
    return (df.assign(Fueltype=df['Fueltype'].str.lower())
              .rename(columns={'Fueltype': 'carrier',
                               'Capacity': 'p_nom',
                               'Duration': 'max_hours',
                               'Set': 'component'}))


def to_pypsa_network(df, network, buslist=None):
   
   # Export a powerplant dataframe to a pypsa.Network(), specify specific buses
    to allocate the plants (buslist).
   
    df = get_obj_if_Acc(df)
    from scipy.spatial import cKDTree as KDTree
    substation_lv_i = network.buses.index[network.buses['substation_lv']]
    substation_lv_i = substation_lv_i.intersection(
        network.buses.reindex(buslist).index)
    kdtree = KDTree(network.buses.loc[substation_lv_i, ['x', 'y']].values)
    df = df.assign(bus=substation_lv_i[kdtree.query(df[['lon',
                                                        'lat']].values)[1]])
    df.Set.replace('CHP', 'PP', inplace=True)
    if 'Duration' in df:
        df['weighted_duration'] = df['Duration'] * df['Capacity']
        df = (df.groupby(['bus', 'Fueltype', 'Set'])
                .aggregate({'Capacity': sum,
                            'weighted_duration': sum}))
        df = df.assign(Duration=df['weighted_duration'] / df['Capacity'])
        df = df.drop(columns='weighted_duration')
    else:
        df = (df.groupby(['bus', 'Fueltype', 'Set'])
                .aggregate({'Capacity': sum}))
    df = df.reset_index()
    df = to_pypsa_names(df)
    df.index = df.bus + ' ' + df.carrier
    network.import_components_from_dataframe(df[df['component'] != 'Store'],
                                             'Generator')
    network.import_components_from_dataframe(df[df['component'] == 'Store'],
                                             'StorageUnit')


def to_TIMES(df=None, use_scaled_capacity=False, baseyear=2015):
    
    #Transform a given dataset into the TIMES format and export as .xlsx.
    
    if df is None:
        from .collection import matched_data
        df = matched_data()
        if df is None:
            raise RuntimeError("The data to be exported does not yet exist.")
    df = df.loc[(df.YearCommissioned.isnull())
                | (df.YearCommissioned <= baseyear)]
    plausible = True

    # Set region via country names by iso3166-2 codes
    if 'Region' not in df:
        pos = [i for i, x in enumerate(df.columns) if x == 'Country'][0]
        df.insert(pos+1, 'Region', np.nan)
    df.Country = df.Country.replace({'Czech Republic': 'Czechia'})
    df.loc[:, 'Region'] = df.Country.apply(lambda c: cget(name=c).alpha_2)
    df = set_denmark_region_id(df)
    regions = sorted(set(df.Region))
    if None in regions:
        raise ValueError("There are rows without a valid country identifier "
                         "in the dataframe. Please check!")

    # add column with TIMES-specific type. The pattern is as follows:
    # 'ConELC-' + Set + '_' + Fueltype + '-' Technology
    df.loc[:, 'Technology'].fillna('', inplace=True)
    if 'TimesType' not in df:
        pos = [i for i, x in enumerate(df.columns) if x == 'Technology'][0]
        df.insert(pos+1, 'TimesType', np.nan)
    df.loc[:, 'TimesType'] = pd.Series('ConELC-' for _ in range(len(df))) +\
        np.where(df.loc[:, 'Set'].str.contains('CHP'), 'CHP', 'PP') +\
        '_' + df.loc[:, 'Fueltype'].map(fueltype_to_abbrev())
    df.loc[(df.Fueltype == 'Wind')
           & (df.Technology.str.contains('offshore', case=False)),
           'TimesType'] += 'F'
    df.loc[(df.Fueltype == 'Wind')
           & ~(df.Technology.str.contains('offshore', case=False)),
           'TimesType'] += 'N'
    df.loc[(df.Fueltype == 'Solar')
           & (df.Technology.str.contains('CSP', case=False)),
           'TimesType'] += 'CSP'
    df.loc[(df.Fueltype == 'Solar')
           & ~(df.Technology.str.contains('CSP', case=False)),
           'TimesType'] += 'SPV'
    df.loc[(df.Fueltype == 'Natural Gas')
           & (df.Technology.str.contains('CCGT', case=False)),
           'TimesType'] += '-CCGT'
    df.loc[(df.Fueltype == 'Natural Gas')
           & ~(df.Technology.str.contains('CCGT', case=False))
           & (df.Technology.str.contains('OCGT', case=False)),
           'TimesType'] += '-OCGT'
    df.loc[(df.Fueltype == 'Natural Gas')
           & ~(df.Technology.str.contains('CCGT', case=False))
           & ~(df['Technology'].str.contains('OCGT', case=False)),
           'TimesType'] += '-ST'
    df.loc[(df.Fueltype == 'Hydro')
           & (df.Technology.str.contains('pumped storage', case=False)),
           'TimesType'] += '-PST'
    df.loc[(df.Fueltype == 'Hydro')
           & (df.Technology.str.contains('run-of-river', case=False))
           & ~(df.Technology.str.contains('pumped storage', case=False)),
           'TimesType'] += '-ROR'
    df.loc[(df.Fueltype == 'Hydro')
           & ~(df.Technology.str.contains('run-of-river', case=False))
           & ~(df.Technology.str.contains('pumped storage', case=False)),
           'TimesType'] += '-STO'

    if None in set(df.TimesType):
        raise ValueError("There are rows without a valid TIMES-Type "
                         "identifier in the dataframe. Please check!")

    # add column with technical lifetime
    if 'Life' not in df:
        pos = [i for i, x in enumerate(df.columns) if x == 'Retrofit'][0]
        df.insert(pos+1, 'Life', np.nan)
    df.loc[:, 'Life'] = df.TimesType.map(timestype_to_life())
    if df.Life.isnull().any():
        raise ValueError("There are rows without a given lifetime in the "
                         "dataframe. Please check!")

    # add column with decommissioning year
    if 'YearRetire' not in df:
        pos = [i for i, x in enumerate(df.columns) if x == 'Life'][0]
        df.insert(pos+1, 'YearRetire', np.nan)
    df.loc[:, 'YearRetire'] = df.loc[:, 'Retrofit'] + df.loc[:, 'Life']
    df = set_known_retire_years(df)

    # Now create empty export dataframe with headers
    columns = ['Attribute', '*Unit', 'LimType', 'Year']
    columns.extend(regions)
    columns.append('Pset_Pn')

    # Loop stepwise through technologies, years and countries
    df_exp = pd.DataFrame(columns=columns)
    cap_column = 'Scaled Capacity' if use_scaled_capacity else 'Capacity'
    row = 0
    for tt, df_tt in df.groupby('TimesType'):
        for yr in range(baseyear, 2055, 5):
            df_exp.loc[row, 'Year'] = yr
            data_regions = df_tt.groupby('Region')
            for reg in regions:
                if reg in data_regions.groups:
                    ct_group = data_regions.get_group(reg)
                    # Here, all matched units existing in the dataset are being
                    # considered. This is needed since there can be units in
                    # the system which are actually already beyond their
                    # assumed technical lifetimes but still online in baseyear.
                    if yr == baseyear:
                        series = ct_group.apply(lambda x: x[cap_column],
                                                axis=1)
                    # Here all matched units that are not retired in yr,
                    # are being filtered.
                    elif yr > baseyear:
                        series = ct_group.apply(lambda x: x[cap_column]
                                                if yr >= x['YearCommissioned']
                                                and yr <= x['YearRetire']
                                                else 0, axis=1)
                    else:
                        message = 'loop yr({}) below baseyear({})'
                        raise ValueError(message.format(yr, baseyear))
                    # Divide the sum by 1000 (MW->GW) and write into export df
                    df_exp.loc[row, reg] = series.sum()/1000.0
                else:
                    df_exp.loc[row, reg] = 0.0
                # Plausibility-Check:
                if (yr > baseyear and (df_exp.loc[row, reg]
                                       > df_exp.loc[row-1, reg])):
                    plausible = False
                    logger.error(
                        "For region '{}' and timestype '{}' the value for "
                        "year {} ({0.000}) is higher than in the year before "
                        "({0.000}).".format(reg, tt, yr, df_exp.loc[row, reg],
                                            df_exp.loc[row-1, reg]))
            df_exp.loc[row, 'Pset_Pn'] = tt
            row += 1
    df_exp.loc[:, 'Attribute'] = 'STOCK'
    df_exp.loc[:, '*Unit'] = 'GW'
    df_exp.loc[:, 'LimType'] = 'FX'

    # Write resulting dataframe to file
    if plausible:
        df_exp.to_excel(_data_out('Export_Stock_TIMES.xlsx'))
    return df_exp


def store_open_dataset():
    from .collection import matched_data, reduce_matched_dataframe
    m = (matched_data(reduced=False)
         .reindex(columns=['CARMA', 'ENTSOE', 'GEO', 'GPD', 'OPSD'], level=1)
         [lambda df: df.Name.notnull().any(1)])
    m.to_csv(_data_out('powerplants_large.csv'))
    m = m.pipe(reduce_matched_dataframe)
    m.to_csv(_data_out('powerplants.csv'))
    return m


def fueltype_to_abbrev():
    
    Return the fueltype-specific abbreviation.
    
    data = {'Bioenergy': 'BIO',
            'Geothermal': 'GEO',
            'Hard Coal': 'COA',
            'Hydro': 'HYD',
            'Lignite': 'LIG',
            'Natural Gas': 'NG',
            'Nuclear': 'NUC',
            'Oil': 'OIL',
            'Other': 'OTH',
            'Solar': '',  # DO NOT delete this entry!
            'Waste': 'WST',
            'Wind': 'WO'}
    return data


def timestype_to_life():
    
    Returns the timestype-specific technical lifetime.
    
    return {'ConELC-PP_COA': 45,
            'ConELC-PP_LIG': 45,
            'ConELC-PP_NG-OCGT': 40,
            'ConELC-PP_NG-ST': 40,
            'ConELC-PP_NG-CCGT': 40,
            'ConELC-PP_OIL': 40,
            'ConELC-PP_NUC': 50,
            'ConELC-PP_BIO': 25,
            'ConELC-PP_HYD-ROR': 200,  # According to A.K. Riekkolas comment,
            'ConELC-PP_HYD-STO': 200,  # these will not retire after 75-100 a,
            'ConELC-PP_HYD-PST': 200,  # but exist way longer at retrofit costs
            'ConELC-PP_WON': 25,
            'ConELC-PP_WOF': 25,
            'ConELC-PP_SPV': 30,
            'ConELC-PP_CSP': 30,
            'ConELC-PP_WST': 30,
            'ConELC-PP_SYN': 5,
            'ConELC-PP_CAES': 40,
            'ConELC-PP_GEO': 30,
            'ConELC-PP_OTH': 5,
            'ConELC-CHP_COA': 45,
            'ConELC-CHP_LIG': 45,
            'ConELC-CHP_NG-OCGT': 40,
            'ConELC-CHP_NG-ST': 40,
            'ConELC-CHP_NG-CCGT': 40,
            'ConELC-CHP_OIL': 40,
            'ConELC-CHP_BIO': 25,
            'ConELC-CHP_WST': 30,
            'ConELC-CHP_SYN': 5,
            'ConELC-CHP_GEO': 30,
            'ConELC-CHP_OTH': 5}

//////////////////////////////heuristics.py/////////////////////////////////////////

from .core import get_config, _package_data, get_obj_if_Acc
from .utils import lookup, get_name

import pandas as pd
import numpy as np
import logging
from six import iteritems
logger = logging.getLogger(__name__)


def extend_by_non_matched(df, extend_by, label=None, query=None,
                          aggregate_added_data=True,
                          config=None, **aggkwargs):
    
    Returns the matched dataframe with additional entries of non-matched
    powerplants of a reliable source.
    
    Parameters
    ----------
    df : Pandas.DataFrame
        Already matched dataset which should be extended
    extend_by : pd.DataFrame | str
        Database which is partially included in the matched dataset, but
        which should be included totally. If str is passed, is will be used
        to call the corresponding data from data.py
    label : str
        Column name of the additional database within the matched dataset, this
        string is used if the columns of the additional database do not
        correspond to the ones of the dataset
    
    from . import data
    from .cleaning import aggregate_units
    df = get_obj_if_Acc(df)

    if config is None:
        config = get_config()

    if isinstance(extend_by, str):
        label = extend_by
        extend_by = getattr(data, extend_by)()
    label = get_name(extend_by) if label is None else label

    if df.columns.nlevels > 1:
        included_ids = df['projectID', label].dropna().sum()
    else:
        included_ids = (df.projectID.dropna().map(lambda d: d.get(label))
                          .dropna().sum())
    if included_ids == 0:
        logger.warning(f'{label} not existent in the matched date, extending'
                    ' by all data entries.')
        included_ids = []

    if query is not None:
        extend_by.query(query, inplace=True)
    extend_by = extend_by.loc[~ extend_by.projectID.isin(included_ids)]
    if aggregate_added_data:
        aggkwargs.update({'save_aggregation': False})
        extend_by = aggregate_units(extend_by, dataset_name=label,
                                    config=config, **aggkwargs)
        extend_by = extend_by.assign(
                projectID=extend_by.projectID.map(lambda x: {label: x}))
    else:
        extend_by = extend_by.assign(
                projectID=extend_by.projectID.map(lambda x: {label: [x]}))

    if df.columns.nlevels > 1:
        return df.append(
                pd.concat([extend_by], keys=[label], axis=1) #, ignore_index=True ??
                .swaplevel(axis=1)
                .reindex(columns=df.columns), ignore_index=True)
    else:
        return df.append(extend_by.reindex(columns=df.columns),
                         ignore_index=True)


def rescale_capacities_to_country_totals(df, fueltypes=None):
    
   # Returns a extra column 'Scaled Capacity' with an up or down scaled capacity
    #in order to match the statistics of the ENTSOe country totals. For every
    #country the information about the total capacity of each fueltype is given.
    #The scaling factor is determined by the ratio of the aggregated capacity of
    #the fueltype within each coutry and the ENTSOe statistics about the
    #fueltype capacity total within each country.
   
  
    df : Pandas.DataFrame
        Data set that should be modified
    fueltype : str or list of strings
        fueltype that should be scaled
    
    from .data import Capacity_stats
    df = get_obj_if_Acc(df)
    df = df.copy()
    if fueltypes is None:
        fueltypes = df.Fueltype.unique()
    if isinstance(fueltypes, str):
        fueltypes = [fueltypes]
    stats_df = lookup(df).loc[fueltypes]
    stats_entsoe = lookup(Capacity_stats()).loc[fueltypes]
    if ((stats_df == 0) & (stats_entsoe != 0)).any().any():
        print('Could not scale powerplants in the countries %s because of no \
              occurring power plants in these countries' %
              stats_df.loc[:, ((stats_df == 0) & (stats_entsoe != 0)).any()]
                      .columns.tolist())
    ratio = (stats_entsoe/stats_df).fillna(1)
    df.loc[:, 'Scaled Capacity'] = df.loc[:, 'Capacity']
    for country in ratio:
        for fueltype in fueltypes:
            df.loc[(df.Country == country) & (df.Fueltype == fueltype),
                   'Scaled Capacity'] *= ratio.loc[fueltype, country]
    return df


def fill_missing_duration(df):
    df = get_obj_if_Acc(df)
    mean_duration = df[df.Set == 'Store'].groupby('Fueltype').Duration.mean()
    df = get_obj_if_Acc(df)
    for store in mean_duration.index:
        df.loc[(df['Set'] == 'Store') & (df['Fueltype'] == store),
               'Duration'] = mean_duration.at[store]
    return df


def extend_by_VRE(df, config=None, base_year=2017, prune_beyond=True):
    
   # Extends a given reduced dataframe by externally given VREs.
    
   
    df : pandas.DataFrame
        The dataframe to be extended
    base_year : int
        
# Needed for deriving cohorts from IRENA's capacity statistics
   
    df : pd.DataFrame
         Extended dataframe
    
    from .data import OPSD_VRE
    df = get_obj_if_Acc(df)
    config = get_config() if config is None else config

    vre = OPSD_VRE(config=config).query('Fueltype != "Hydro"')\
            .reindex(columns=config['target_columns'])
    return df.append(vre, sort=False)



    df = df.copy()
    # Drop Solar (except CSP), Wind and Bioenergy which are to be replaced
    df = df[~(((df.Fueltype == 'Solar') & (df.Technology != 'CSP')) |
              (df.Fueltype == 'Wind') | (df.Fueltype == 'Bioenergy'))]
    cols = df.columns
  # Take CH, DE, DK values from OPSD
    logger.info('Read OPSD_VRE dataframe...')
    vre_CH_DE_DK = OPSD_VRE().loc[lambda x: x.Fueltype.isin(['Solar', 'Wind',
                                                             'Bioenergy'])]
    vre_DK = vre_CH_DE_DK[vre_CH_DE_DK.Country == 'Denmark']
    vre_CH_DE = vre_CH_DE_DK[vre_CH_DE_DK.Country != 'Denmark']
    logger.info('Aggregate CH+DE by commyear')
    vre_CH_DE = aggregate_VRE_by_commyear(vre_CH_DE)
    vre_CH_DE.loc[:, 'File'] = 'renewable_power_plants.sqlite'
    # Take other countries from IRENA stats without:
    # DE, DK_Wind+Solar+Hydro, CH_Bioenergy
    logger.info('Read IRENA_stats dataframe...')
    vre = IRENA_stats().loc[lambda x: x.Fueltype.isin(['Solar', 'Wind',
                                                    'Bioenergy'])] 
     vre = vre[~(vre.Country == 'Germany')]
    vre = vre[~((vre.Country == 'Denmark') & ((vre.Fueltype == 'Wind') |
             (vre.Fueltype == 'Solar') | (vre.Fueltype == 'Hydro')))]
    vre = vre[~((vre.Country == 'Switzerland') &
                (vre.Fueltype == 'Bioenergy'))]
    # Drop IRENA's CSP. This data seems to be outdated!
    vre = vre[~(vre.Technology == 'CSP')]
     vre = derive_vintage_cohorts_from_statistics(vre, base_year=base_year)
    vre.loc[:, 'File'] = 'IRENA_CapacityStatistics2017.csv'
    # Concatenate
    logger.info('Concatenate...')
    cc = pd.concat([df, vre_DK, vre_CH_DE, vre], ignore_index=True, sort=False)
    cc = cc.loc[:, cols]
    if prune_beyond:
        cc = cc[(cc.YearCommissioned <= base_year) |
                (cc.YearCommissioned.isnull())]
    cc.reset_index(drop=True, inplace=True)
    return cc;


def fill_missing_commyears(df):
    
    #Fills the empty commissioning years with averages.
 
    df = get_obj_if_Acc(df)
    df = df.copy()
    # 1st try: Fill with both country- and fueltypespecific averages
    df.YearCommissioned.fillna(df.groupby(['Country', 'Fueltype'])
                                 .YearCommissioned
                                 .transform('mean'), inplace=True)
    # 2nd try: Fill remaining with only fueltype-specific average
    df.YearCommissioned.fillna(df.groupby(['Fueltype']).YearCommissioned
                                 .transform('mean'), inplace=True)
    # 3rd try: Fill remaining with only country-specific average
    df.YearCommissioned.fillna(df.groupby(['Country']).YearCommissioned
                                 .transform('mean'), inplace=True)
    if df.YearCommissioned.isnull().any():
        count = len(df[df.YearCommissioned.isnull()])
        raise(ValueError('''There are still *{0}* empty values for
                            'YearCommissioned' in the DataFrame. These should
                            be either be filled manually or dropped to
                            continue.'''.format(count)))
    df.loc[:, 'YearCommissioned'] = df.YearCommissioned.astype(int)
    df.Retrofit.fillna(df.YearCommissioned.astype(int), inplace=True)
    return df


def fill_missing_decommyears(df, config=None):
    
    #Function which sets/fills a column 'YearDecommissioning' with roughly
    #estimated values for decommissioning years, based on the estimated lifetimes
    #per `Fueltype` given in the config and corresponding commissioning years.
    #Note that the latter is filled up using `fill_missing_commyears`.
    
    df = get_obj_if_Acc(df)
    if config is None: config = get_config()
    if 'YearDecommissioning' not in df:
        df = df.reindex(columns = list(df.columns) + ['YearDecommissioning'])
    lifetime = df.Fueltype.map(config['fuel_to_lifetime'])
    df = fill_missing_commyears(df)
    df['YearDecommissioning'] = (df.YearDecommissioning
                                 .fillna(df[['YearCommissioned', 'Retrofit']].max(1)
                                         + lifetime).astype(int))
    return df


def aggregate_VRE_by_commyear(df, target_fueltypes=None, agg_geo_by=None):
    
    #Aggregate the vast number of VRE (e.g. vom data.OPSD_VRE()) units to one
    //specific (Fueltype + Technology) cohorte per commissioning year.
    
    
    df : pd.DataFrame
        DataFrame containing the data to aggregate
    target_fueltypes : list
        list of fueltypes to be aggregated (Others are cutted!)
    agg_by_geo : str
        How to deal with lat/lon positions. Allowed:
            NoneType : Do not show geoposition at all
            'mean'   : Average geoposition
            'wm'     : Average geoposition weighted by capacity
    
    df = df.copy()
    if agg_geo_by is None:
        f = {'Capacity': ['sum']}
    elif agg_geo_by == 'mean':
        f = {'Capacity': ['sum'], 'lat': ['mean'], 'lon': ['mean']}
    elif agg_geo_by == 'wm':
        # TODO: This does not work yet, when NaNs are in lat/lon columns.
        wm = lambda x: np.average(x, weights=df.loc[x.index, 'Capacity'])
        f = {'Capacity': ['sum'],
             'lat': {'weighted mean': wm},
             'lon': {'weighted mean': wm}}
    else:
        raise TypeError("Value given for `agg_geo_by` is '{}' but must be either \
                        'NoneType' or 'mean' or 'wm'.".format(agg_geo_by))

    if target_fueltypes is None:
        target_fueltypes = ['Wind', 'Solar', 'Bioenergy']
    df = df[df.Fueltype.isin(target_fueltypes)]
    df = fill_missing_commyears(df)
    df.Technology.fillna('-', inplace=True)
    df = (df.groupby(['Country', 'YearCommissioned', 'Fueltype', 'Technology'])
            .agg(f).reset_index().replace({'-': np.NaN}))
    df.columns = df.columns.droplevel(level=1)
    return df.assign(Set='PP',
                     Retrofit=df.YearCommissioned)


def derive_vintage_cohorts_from_statistics(df, base_year=2015, config=None):
    
   # This function assumes an age-distribution for given capacity statistics
    #and returns a df, containing how much of capacity has been built for every
    year.
    
    def setInitial_Flat(mat, df, life):
        y_start = int(df.index[0])
        height_flat = float(df.loc[y_start].Capacity) / life
        for y in range(int(mat.index[0]), y_start+1):
            y_end = min(y+life-1, mat.columns[-1])
            mat.loc[y, y:y_end] = height_flat
        return mat

    def setInitial_Triangle(mat, df, life):
        y_start = int(df.index[0])
        years = range(y_start-life+1, y_start+1)
        height_flat = float(df.loc[y_start].Capacity) / life
        # decrement per period, 'slope' of the triangle
        decr = 2.0*height_flat/life
        # height of triangle at right side
        height_tri = 2.0*height_flat - decr/2.0
        series = [(height_tri - i*decr) for i in range(0, life)][::-1]
        dic = dict(zip(years, series))           # create dictionary
        for y in range(int(mat.index[0]), y_start+1):
            y_end = min(y+life-1, mat.columns[-1])
            mat.loc[y, y:y_end] = dic[y]
        return mat

    def setHistorical(mat, df, life):
        # Base year was already handled in setInitial()->Start one year later.
        year = df.index[1]
        while year <= df.index.max():
            if year in df.index:
                addition = df.loc[year].Capacity - mat.loc[:, year].sum()
                if addition >= 0:
                    mat.loc[year, year:year+life-1] = addition
                else:
                    mat.loc[year, year:year+life-1] = 0
                    mat = reduceVintages(addition, mat, life, year)
            else:
                mat.loc[year, year:year+life-1] = 0
            year += 1
        return mat

    def reduceVintages(addition, mat, life, y_pres):
        for year in mat.index:
            val_rem = float(mat.loc[year, y_pres])
            print ('In year %i are %.2f units left from year %i, while '
                   'addition delta is %.2f' % (y_pres, val_rem, year,
                                               addition))
            if val_rem > 0:
                if abs(addition) > val_rem:
                    mat.loc[year, y_pres:year+life-1] = 0
                    addition += val_rem
                else:
                    mat.loc[year, y_pres:year+life-1] = val_rem + addition
                    break
        return mat

    if config is None:
        config = get_config()

    dfe = pd.DataFrame(columns=df.columns)
    for c, df_country in df.groupby(['Country']):
        for tech, dfs in df_country.groupby(['Technology']):
            dfs.set_index('YearCommissioned', drop=False, inplace=True)
            y_start = int(dfs.index[0])
            y_end = int(dfs.index[-1])
            life = config['fuel_to_lifetime'][dfs.Fueltype.iloc[0]]
            mat = (pd.DataFrame(columns=range(y_start-life+1, y_end+life),
                                index=range(y_start-life+1, y_end))
                     .astype(np.float))
            if dfs.Fueltype.iloc[0] in ['Solar', 'Wind', 'Bioenergy',
                                        'Geothermal']:
                mat = setInitial_Triangle(mat, dfs, life)
            else:
                mat = setInitial_Flat(mat, dfs, life)
            if y_end > y_start:
                mat = setHistorical(mat, dfs, life)
            add = pd.DataFrame(columns=dfs.columns)
            add.Capacity = list(mat.loc[:, base_year])
            add.Year = mat.index.tolist()
            add.Technology = tech
            add.Country = c
            add.Fueltype = dfs.Fueltype.iloc[0]
            add.Set = dfs.Set.iloc[0]
            dfe = pd.concat([dfe, add[add.Capacity > 0.0]], ignore_index=True)
    dfe.Year = dfe.Year.apply(pd.to_numeric)
    dfe.rename(columns={'Year': 'YearCommissioned'}, inplace=True)
    dfe = dfe.assign(Retrofit=dfe.YearCommissioned)
    return dfe[~np.isclose(dfe.Capacity, 0)]


def set_denmark_region_id(df):
   
   
   # Used to set the Region column to DKE/DKW (East/West) for electricity models.based on lat,lon-coordinates and a heuristic for unknowns.
    
    if 'Region' not in df:
        pos = [i for i, x in enumerate(df.columns) if x == 'Country'][0]
        df.insert(pos+1, 'Region', np.nan)
    else:
        if ('DKE' in set(df.Region)) | ('DKW' in set(df.Region)):
            return df
        df.loc[(df.Country == 'Denmark'), 'Region'] = np.nan
    # TODO: This does not work yet.
        # import geopandas as gpd
        # df = gpd.read_file('/tmp/ne_10m_admin_0_countries/')
        # df = df.query("ISO_A2 != '-99'").set_index('ISO_A2')
        # Point(9, 52).within(df.loc['DE', 'geometry'])
    # Workaround:
    df.loc[(df.Country == 'Denmark') & (df.lon >= 10.96), 'Region'] = 'DKE'
    df.loc[(df.Country == 'Denmark') & (df.lon < 10.96), 'Region'] = 'DKW'
    df.loc[df.Name.str.contains('Jegerspris', case=False).fillna(False),
           'Region'] = 'DKE'
    df.loc[df.Name.str.contains('Jetsmark', case=False).fillna(False),
           'Region'] = 'DKW'
    df.loc[df.Name.str.contains('Fellinggard', case=False).fillna(False),
           'Region'] = 'DKW'
    # Copy the remaining ones without Region and handle in copy
    dk_o = (df.loc[(df.Country == 'Denmark') & (df.Region.isnull())]
              .reset_index(drop=True))
    dk_o.loc[:, 'Capacity'] *= 0.5
    dk_o.loc[:, 'Region'] = 'DKE'
    # Handle remaining in df
    df.loc[(df.Country == 'Denmark') & (df.Region.isnull()), 'Capacity'] *= 0.5
    df.loc[(df.Country == 'Denmark') & (df.Region.isnull()), 'Region'] = 'DKW'
    # Concat
    df = pd.concat([df, dk_o], ignore_index=True)
    return df


def remove_oversea_areas(df, lat=[36, 72], lon=[-10.6, 31]):
    
    #Remove plants outside continental Europe such as the Canarian Islands etc.
    
    df = df.loc[(df.lat.isnull() | df.lon.isnull()) |
                ((df.lat >= lat[0]) & (df.lat <= lat[1]) &
                 (df.lon >= lon[0]) & (df.lon <= lon[1]))]
    return df


def gross_to_net_factors(reference='opsd', aggfunc='median',
                         return_entire_data=False):
    
    from .cleaning import clean_technology
    if reference == 'opsd':
        from .data import OPSD
        reference = OPSD(rawDE=True)
    df = reference.copy()
    df = df[df.capacity_gross_uba.notnull() & df.capacity_net_bnetza.notnull()]
    df.loc[:, 'ratio'] = df.capacity_net_bnetza / df.capacity_gross_uba
    df = df[df.ratio <= 1.0]  # drop obvious data errors
    if return_entire_data:
        return df
    else:
        df.energy_source_level_2.fillna(value=df.fuel, inplace=True)
        df.replace(dict(energy_source_level_2={
                'Biomass and biogas': 'Bioenergy',
                'Fossil fuels': 'Other',
                'Mixed fossil fuels': 'Other',
                'Natural gas': 'Natural Gas',
                'Non-renewable waste': 'Waste',
                'Other bioenergy and renewable waste': 'Bioenergy',
                'Other or unspecified energy sources': 'Other',
                'Other fossil fuels': 'Other',
                'Other fuels': 'Other'}), inplace=True)
        df.rename(columns={'technology': 'Technology'}, inplace=True)
        df = (clean_technology(df)
              .assign(energy_source_level_2=lambda df:
                      df.energy_source_level_2.str.title()))
        ratios = df.groupby(['energy_source_level_2',
                             'Technology']).ratio.mean()
        return ratios


def scale_to_net_capacities(df, is_gross=True, catch_all=True):
    df = get_obj_if_Acc(df)
    if is_gross:
        factors = gross_to_net_factors()
        for ftype, tech in factors.index.values:
            df.loc[(df.Fueltype == ftype) & (df.Technology == tech),
                   'Capacity'] *= factors.loc[(ftype, tech)]
        if catch_all:
            for ftype in factors.index.levels[0]:
                techs = factors.loc[ftype].index.tolist()
                df.loc[(df.Fueltype == ftype) & (~df.Technology.isin(techs)),
                       'Capacity'] *= factors.loc[ftype].mean()
        return df
    else:
        return df


def PLZ_to_LatLon_map():
    return pd.read_csv(_package_data('PLZ_Coords_map.csv'), index_col='PLZ')


def set_known_retire_years(df):
    
    if 'YearRetire' not in df:
        df['YearRetire'] = np.nan

    YearRetire = {
        'Grafenrheinfeld': 2015,
        'Philippsburg': 2019,
        'Brokdorf': 2021,
        'Grohnde': 2021,
        'Gundremmingen': 2021,
        'Emsland': 2022,
        'Isar': 2022,
        'Neckarwestheim': 2022
    }

    ppl_de_nuc = pd.DataFrame(df.loc[(df.Country == 'Germany') &
                                     (df.Fueltype == 'Nuclear'),
                                     ['Name', 'YearRetire']])
    for name, year in iteritems(YearRetire):
        name_match_b = ppl_de_nuc.Name.str.contains(name, case=False, na=False)
        if name_match_b.any():
            ppl_de_nuc.loc[name_match_b, 'YearRetire'] = year
        else:
            logger.warn("'{}' was not found in given DataFrame.".format(name))
    df.loc[ppl_de_nuc.index, 'YearRetire'] = ppl_de_nuc['YearRetire']
    return df

/////////// Plot.py/////////

import math
import numpy as np
import pandas as pd
#import collections
import matplotlib.pyplot as plt
#import matplotlib.patches as mpatches
from matplotlib.patches import Circle, Ellipse
from matplotlib.legend_handler import HandlerPatch
from matplotlib import rcParams, cycler
from matplotlib.lines import Line2D
import seaborn as sns

from .core import get_config, get_obj_if_Acc
from .utils import lookup, set_uncommon_fueltypes_to_other, to_list_if_other

import logging
logger = logging.getLogger(__name__)


cartopy_present = True
try:
    import cartopy.crs as ccrs
    import cartopy
except (ModuleNotFoundError, ImportError):
    cartopy_present = False

if not cartopy_present:
    logger.warn('Cartopy not existent.')


def fueltype_stats(df):
    df = get_obj_if_Acc(df)
    stats = lookup(set_uncommon_fueltypes_to_other(df), by='Fueltype')
    plt.pie(stats, colors=stats.index.to_series()
                               .map(get_config()['fuel_to_color'])
                               .fillna('gray'),
            labels=stats.index, autopct='%1.1f%%')


def powerplant_map(df, scale=1e2, european_bounds=True, fillcontinents=False,
                   legendscale=1, resolution=True, **kwargs):

    df = get_obj_if_Acc(df)
    # TODO: add reference circle in legend
    figsize = kwargs.get('figsize', (11, 9))
    with sns.axes_style('darkgrid'):
        df = set_uncommon_fueltypes_to_other(df)
        shown_fueltypes = df.Fueltype.unique()
        df = df[df.lat.notnull()]
        sub_kw = {'projection': ccrs.PlateCarree()} if cartopy_present else {}
        fig, ax = plt.subplots(figsize=figsize, subplot_kw=sub_kw)

        ax.scatter(df.lon, df.lat, s=df.Capacity/scale,
                   c=df.Fueltype.map(get_config()['fuel_to_color']),
                   edgecolor='face', facecolor='face')

        legendcols = (pd.Series(get_config()['fuel_to_color'])
                        .reindex(shown_fueltypes))
        handles = sum(legendcols.apply(lambda x:
                      make_legend_circles_for([10.], scale=scale*legendscale,
                                              facecolor=x)).tolist(), [])
        ax.legend(handles, legendcols.index,
                   handler_map=make_handler_map_to_scale_circles_as_in(ax),
                   markerscale=1,
                   ncol=kwargs.get('ncol', 2),
                   loc=kwargs.get('loc', "upper left"),
                   frameon=True, fancybox=True, edgecolor='w',
                   facecolor='w', framealpha=1)

        ax.set_xlabel('')
        ax.set_ylabel('')
        if european_bounds:
            ax.set_xlim(-13, 34)
            ax.set_ylim(35, 72)
        draw_basemap(ax=ax, resolution=resolution,
                     fillcontinents=fillcontinents)
        ax.set_facecolor('w')

        fig.tight_layout(pad=0.5)
        if cartopy_present:
            ax.outline_patch.set_visible(False)
        return fig, ax


# This approach is an alternative to bar_comparison_countries_fueltypes()

def fueltype_and_country_totals_bar(dfs, keys=None, figsize=(18, 8)):
    dfs = get_obj_if_Acc(dfs)
    dfs = to_list_if_other(dfs)
    df = lookup(dfs, keys)
    countries = df.index.get_level_values('Country').unique()
    n = len(countries)
    subplots = gather_nrows_ncols(n)
    fig, ax = plt.subplots(*subplots, figsize=figsize)

    if sum(subplots) > 2:
        ax_iter = ax.flat
    else:
        ax_iter = np.array(ax).flat
    for country in countries:
        ax = next(ax_iter)
        df.loc[country].plot.bar(ax=ax, sharex=True, legend=None)
        ax.set_xlabel('')
        ax.ticklabel_format(axis='y', style='sci', scilimits=(-2, 2))
        ax.set_title(country)
    handles, labels = ax.get_legend_handles_labels()
    fig.tight_layout(pad=1)
    fig.legend(handles, labels, loc=9, ncol=2, bbox_to_anchor=(0.53, 0.99))
    fig.subplots_adjust(top=0.9)
    return fig, ax


def fueltype_totals_bar(dfs, keys=None, figsize=(7, 4), unit='GW',
                        last_as_marker=False, axes_style='whitegrid',
                        exclude=[], **kwargs):
    dfs = get_obj_if_Acc(dfs)
    dfs = to_list_if_other(dfs)
    dfs = [df[~df.Fueltype.isin(exclude)] for df in dfs]
    with sns.axes_style(axes_style):
        fig, ax = plt.subplots(1, 1, figsize=figsize)
        if last_as_marker:
            as_marker = dfs[-1]
            dfs = dfs[:-1]
            as_marker_key = keys[-1]
            keys = keys[:-1]
        fueltotals = lookup(dfs, keys=keys, by='Fueltype', unit=unit)
        fueltotals.plot(kind="bar", ax=ax, legend='reverse',
                        edgecolor='none', rot=90, **kwargs)
        if last_as_marker:
            fueltotals = lookup(as_marker, keys=as_marker_key, by='Fueltype',
                                unit=unit)
            fueltotals.plot(ax=ax, label=as_marker_key, markeredgecolor='none',
                            rot=90, marker='D', linestyle='None',
                            markerfacecolor='darkslategray', **kwargs)
        ax.legend(loc=0)
        ax.set_ylabel(r'Capacity [$%s$]' % unit)
        ax.xaxis.grid(False)
        fig.tight_layout(pad=0.5)
        return fig, ax


def country_totals_hbar(dfs, keys=None, exclude_fueltypes=['Solar', 'Wind'],
                        figsize=(7, 5), unit='GW', axes_style='whitegrid'):
    with sns.axes_style(axes_style):
        fig, ax = plt.subplots(1, 1, figsize=figsize)
        countrytotals = lookup(dfs, keys=keys, by='Country',
                               exclude=exclude_fueltypes, unit=unit)
        countrytotals[::-1][1:].plot(kind="barh", ax=ax, legend='reverse',
                                     edgecolor='none')
        ax.set_xlabel('Capacity [%s]' % unit)
        ax.yaxis.grid(False)
        ax.set_ylabel('')
        fig.tight_layout(pad=0.5)
        return fig, ax


def factor_comparison(dfs, keys=None, figsize=(12, 9)):
    with sns.axes_style('whitegrid'):
        dfs = [set_uncommon_fueltypes_to_other(df) for df in dfs]
        compare = lookup(dfs, keys=keys, exclude=['Solar', 'Wind']).fillna(0.)
        compare = compare.append(
            pd.concat([compare.groupby(level='Country').sum()],
                      keys=['Total']).swaplevel()).sort_index()/1000
        n_countries, n_fueltypes = compare.index.levshape
        c = [get_config()['fuel_to_color'][i] for i in compare.index.levels[1]]
        rcParams["axes.prop_cycle"] = cycler(color=c)

        # where both are zero,
        compare[compare.sum(1) < 0.5] = np.nan

        fig, ax = plt.subplots(1, 1, figsize=figsize)
        compare = (compare.unstack('Country').swaplevel(axis=1)
                   .sort_index(axis=1).reindex(columns=keys, level=1))
        compare.T.plot(ax=ax, markevery=(0, 2),
                       style='o', markersize=5)
        compare.T.plot(ax=ax, markevery=(1, 2),
                       style='s', legend=None, markersize=4.5)

        lgd = ax.get_legend_handles_labels()

        for i, j in enumerate(compare.columns.levels[0]):
            ax.plot(np.array([0, 1]) + (2*i), compare[j].T)

        indexhandles = [Line2D([0.4, 0.6], [0.4, 0.6], marker=m, linewidth=0.,
                        markersize=msize, color='w', markeredgecolor='k',
                        markeredgewidth=0.5)
                        for m, msize in [['o', 5.], ['s', 4.5]]]
        ax.add_artist(ax.legend(handles=indexhandles, labels=keys))
        ax.legend(handles=lgd[0][:len(c)], labels=lgd[1][:len(c)],
                  title=False, loc=2)

        ax.set_xlim(-1, n_countries*2 + 1)
        ax.xaxis.grid(False)
        ax.set_xticks(np.linspace(0.5, n_countries*2 - 1.5, n_countries))
        ax.set_xticklabels(compare.columns.levels[0].values, rotation=90)
        ax.set_xlabel('')
        ax.set_ylabel('Capacity [GW]')
        fig.tight_layout(pad=0.5)
        return fig, ax


def boxplot_gross_to_net(axes_style='darkgrid', **kwargs):
   
    from .heuristics import gross_to_net_factors as gtn
    with sns.axes_style(axes_style):
        df = (gtn(return_entire_data=True)
              .loc[lambda df: df.energy_source_level_2 != 'Hydro'])
        df.loc[:, 'FuelTech'] = (df.energy_source_level_2
                                 + '\n(' + df.technology + ')')
        df = df.groupby('FuelTech').filter(lambda x: len(x) >= 10)
        dfg = df.groupby('FuelTech')
        # plot
        fig, ax = plt.subplots(**kwargs)
        df.boxplot(ax=ax, column='ratio', by='FuelTech', rot=90,
                   showmeans=True)
        ax.title.set_visible(False)
        ax.set_ylabel('Ratio of gross/net [$-$]')
        ax.xaxis.label.set_visible(False)
        ax.set_ylabel('Net / Gross')
        ax2 = ax.twiny()
        ax2.set_xlim(ax.get_xlim())
        ax2.set_xticks([i+1 for i in range(len(dfg))])
        ax2.set_xticklabels(['$n$=%d' % (len(v)) for k, v in dfg])
        fig.suptitle('')
        return fig, ax


def boxplot_matchcount(df):
    
    # Mend needed data
    df.loc[:, 'Matches'] = df.projectID.apply(len)

    # Plot
    fig, ax = plt.subplots(figsize=(8, 4.5))
    df.boxplot(ax=ax, column='Capacity', by='Matches', showmeans=True)
    ax.title.set_visible(False)
    ax.set_xlabel('Number of datasets participating in match [$-$]')
    ax.set_ylabel('Capacity [$MW$]')
    fig.suptitle('')
    return fig


def make_handler_map_to_scale_circles_as_in(ax,
                                            dont_resize_actively=False):
    fig = ax.get_figure()

    def axes2pt():
        return np.diff(ax.transData.transform([(0, 0), (1, 1)]),
                       axis=0)[0] * (72./fig.dpi)
    ellipses = []
    if not dont_resize_actively:
        def update_width_height(event):
            dist = axes2pt()
            for e, radius in ellipses:
                e.width, e.height = 2. * radius * dist
        fig.canvas.mpl_connect('resize_event', update_width_height)
        ax.callbacks.connect('xlim_changed', update_width_height)
        ax.callbacks.connect('ylim_changed', update_width_height)

    def legend_circle_handler(legend, orig_handle, xdescent, ydescent,
                              width, height, fontsize):
        w, h = 2. * orig_handle.get_radius() * axes2pt()
        e = Ellipse(xy=(0.5*width - 0.5*xdescent,
                        0.5*height - 0.5*ydescent),
                    width=w, height=w)
        ellipses.append((e, orig_handle.get_radius()))
        return e
    return {Circle: HandlerPatch(patch_func=legend_circle_handler)}


def make_legend_circles_for(sizes, scale=1.0, **kw):
    return [Circle((0, 0), radius=(s/scale)**0.5, **kw) for s in sizes]


def draw_basemap(resolution=True, ax=None, country_linewidth=0.3,
                 coast_linewidth=0.4, zorder=None, fillcontinents=True,
                 **kwds):

    if cartopy_present:
        if ax is None:
            ax = plt.gca(projection=ccrs.PlateCarree())
        resolution = '50m' if isinstance(resolution, bool) else resolution
        assert resolution in ['10m', '50m', '110m'],\
            "Resolution has to be one of '10m', '50m', '110m'."
        ax.set_extent(ax.get_xlim() + ax.get_ylim(),
                      crs=ccrs.PlateCarree())
        ax.coastlines(linewidth=0.4, zorder=-1, resolution=resolution)
        border = cartopy.feature.BORDERS.with_scale(resolution)
        ax.add_feature(border, linewidth=0.3)
        ax.outline_patch.set_visible(False)
        if fillcontinents:
            land = cartopy.feature.LAND.with_scale(resolution)
            ax.add_feature(land, facecolor='lavender', alpha=0.25)


def gather_nrows_ncols(x, orientation='landscape'):
    
    #Derives [nrows, ncols] based on x plots, so that a subplot looks nicely.
    
    x : int, Number of subplots between [0, 42]
    
    def calc(n, m):
        if n <= 0:
            n = 1
        if m <= 0:
            m = 1
        while (n*m < x):
            m += 1
        return n, m

    if not isinstance(x, int):
        raise ValueError('An integer needs to be passed to this function.')
    elif x <= 0:
        raise ValueError('The given number of subplots is less or equal zero.')
    elif x > 42:
        raise ValueError("Are you sure that you want to put more than 42 "
                         "subplots in one diagram?\n You better don't, it "
                         "looks squeezed. Otherwise adapt the code.")
    k = math.sqrt(x)
    if k.is_integer():
        return [int(k), int(k), 0]  # Square format!
    else:
        k = int(math.floor(k))
        # Solution 1
        n, m = calc(k, k+1)
        sol1 = {'n': n, 'm': m, 'dif': (m*n) - x}
        # Solution 2:
        n, m = calc(k-1, k+1)
        sol2 = {'n': n, 'm': m, 'dif': (m*n) - x}
        if (((sol1['dif'] <= sol2['dif']) & (sol1['n'] >= 2))
                | (x in [7, 13, 14])):
            n, m = [sol1['n'], sol1['m']]
        else:
            n, m = [sol2['n'], sol2['m']]
        remainder = m*n - x
        if orientation == 'landscape':
            return n, m, remainder
        elif orientation == 'portrait':

            
////////////////////utils.py///////////////////////////////////


from __future__ import print_function, absolute_import

from .core import get_config, _data_in, _package_data, logger, get_obj_if_Acc
import os
import time
import pandas as pd
import six
import pycountry as pyc
import numpy as np
import multiprocessing
from ast import literal_eval as liteval


def lookup(df, keys=None, by='Country, Fueltype', exclude=None, unit='MW'):
    
    df = get_obj_if_Acc(df)
    if unit == 'GW':
        scaling = 1000.
    elif unit == 'MW':
        scaling = 1.
    else:
        raise(ValueError("unit has to be MW or GW"))

    def lookup_single(df, by=by, exclude=exclude):
        df = read_csv_if_string(df)
        if isinstance(by, str):
            by = by.replace(' ', '').split(',')
        if exclude is not None:
            df = df[~df.Fueltype.isin(exclude)]
        return df.groupby(by).Capacity.sum()

    if isinstance(df, list):
        if keys is None:
            keys = [get_name(d) for d in df]
        dfs = pd.concat([lookup_single(a) for a in df], axis=1,
                         keys=keys, sort=False)
        dfs = dfs.fillna(0.)
        return (dfs/scaling).round(3)
    else:
        return (lookup_single(df)/scaling).fillna(0.).round(3)


def parse_if_not_stored(name, update=False, config=None,
                        parse_func=None, **kwargs):
    if config is None:
        config = get_config()
    df_config = config[name]
    path = _data_in(df_config['fn'])

    if not os.path.exists(path) or update:
        if parse_func is None:
            logger.info(f'Retrieving data from {df_config["url"]}')
            data = pd.read_csv(df_config['url'], **kwargs)
        else:
            data = parse_func()
        data.to_csv(path)
    else:
        data = pd.read_csv(path, **kwargs)
    return data


def config_filter(df, name=None, config=None):
                        
                        df = get_obj_if_Acc(df)

    if config is None:
        config = get_config()
    # individual filter from config.yaml
    if name is not None:
        queries = {k: v for source in config['matching_sources']
                   for k, v in to_dict_if_string(source).items()}
        if name in queries and queries[name] is not None:
            df = df.query(queries[name])
    countries = config['target_countries']
    fueltypes = config['target_fueltypes']
    return (df.query("Country in @countries and Fueltype in @fueltypes")
            .reindex(columns=config['target_columns'])
            .reset_index(drop=True))


def correct_manually(df, name, config=None):
                        
                        from .data import data_config
    if config is None:
        config = get_config()

    corrections = pd.read_csv(_package_data('manual_corrections.csv'),
                               encoding='utf-8',
                               parse_dates=['last_update'])

    if name in corrections:
        corrections = corrections[corrections[name].notnull()].set_index(name)
    else:
        return df.reindex(columns=config['target_columns'])
    if len(corrections) == 0:
        return df.reindex(columns=config['target_columns'])
    source_file = data_config[name]['source_file']
    # assume OPSD files are updated on the same time
    if isinstance(source_file, list):
        source_file = source_file[0]
    outdated = (pd.Timestamp(time.ctime(os.path.getmtime(source_file)))
                > corrections.last_update).any()
    if outdated:
        logger.warning('Manual corrections in {0} for file {1} older than last'
                       ' update of the source file, please update your manual '
                       'corrections.'.format(os.path.abspath(
                               _data_in('manual_corrections.csv')), name))
    df = df.set_index('projectID').copy()
    df.update(corrections)
    return df.reset_index().reindex(columns=config['target_columns'])


def set_uncommon_fueltypes_to_other(df, fillna_other=True, config=None,
                                    **kwargs):
                        
                         config = get_config() if config is None else config
    df = get_obj_if_Acc(df)

    default = ['Bioenergy', 'Geothermal', 'Mixed fuel types',
               'Electro-mechanical', 'Hydrogen Storage']
    fueltypes = kwargs.get('fueltypes', default)
    df.loc[df.Fueltype.isin(fueltypes), 'Fueltype'] = 'Other'
    if fillna_other:
        df = df.fillna({'Fueltype': 'Other'})
    return df


def read_csv_if_string(df):
   
    Convenience function to import powerplant data source if a string is given.
   
    from . import data
    if isinstance(data, six.string_types):
        df =getattr(data, df)()
    return df


def to_categorical_columns(df):
   
    Helper function to set datatype of columns 'Fueltype', 'Country', 'Set',
    'File', 'Technology' to categorical.

    cols = ['Fueltype', 'Country', 'Set', 'File']
    cats = {'Fueltype': get_config()['target_fueltypes'],
            'Country': get_config()['target_countries'],
            'Set': get_config()['target_sets']}
    return df.assign(**{c: df[c].astype('category') for c in cols})\
             .assign(**{c: lambda df: df[c].cat.set_categories(v)
                     for c,v in cats.items()})


def set_column_name(df, name):
    
    df.columns.name = name
    return df


def get_name(df):
    
    if df.columns.name is None:
        return 'unnamed data'
    else:
        return df.columns.name


def to_list_if_other(obj):

    if not isinstance(obj, list):
        return [obj]
    else:
        return obj


def to_dict_if_string(s):
    
    if isinstance(s, str):
        return {s: None}
    else:
        return s


def projectID_to_dict(df):
    
    if df.columns.nlevels > 1:
        return df.assign(projectID=(df.projectID.stack().dropna().apply(
                lambda ds: liteval(ds)).unstack()))
    else:
        return df.assign(projectID=df.projectID.apply(lambda x: liteval(x)))

                       def select_by_projectID(df, projectID, dataset_name=None):
    
    df = get_obj_if_Acc(df)

    if isinstance(df.projectID.iloc[0], str):
        return df.query("projectID == @projectID")
    else:
        return df[df['projectID'].apply(lambda x:
                  projectID in sum(x.values(), []))]


def update_saved_matches_for_(name):
                        
                         from .collection import collect
    from .matching import compare_two_datasets
    df = collect(name, use_saved_aggregation=False)
    dfs = [ds for ds in get_config()['matching_sources'] if ds != name]
    for to_match in dfs:
        compare_two_datasets([collect(to_match), df], [to_match, name])


def fun(f, q_in, q_out):
    
    while True:
        i, x = q_in.get()
        if i is None:
            break
        q_out.put((i, f(x)))


def parmap(f, arg_list, config=None):
                        
if arglist : list
                    if config is None:
        config = get_config()
    if config['parallel_duke_processes']:
        nprocs = min(multiprocessing.cpu_count(), config['process_limit'])
        logger.info('Run process with {} parallel threads.'.format(nprocs))
        q_in = multiprocessing.Queue(1)
        q_out = multiprocessing.Queue()

        proc = [multiprocessing.Process(target=fun, args=(f, q_in, q_out))
                for _ in range(nprocs)]
        for p in proc:
            p.daemon = True
            p.start()

        sent = [q_in.put((i, x)) for i, x in enumerate(arg_list)]
        [q_in.put((None, None)) for _ in range(nprocs)]
        res = [q_out.get() for _ in range(len(sent))]

        [p.join() for p in proc]

        return [x for i, x in sorted(res)]
    else:
        return list(map(f, arg_list))



country_map = pd.read_csv(_package_data('country_codes.csv'))\
                .replace({'name': {'Czechia': 'Czech Republic'}})

def country_alpha2(country):  # Convenience function for converting country name into alpha 2 codes
                        
                        
                        if not isinstance(country, str):
        return ''
    try:
        return pyc.countries.get(name=country).alpha_2
    except KeyError:
        return ''


def convert_alpha2_to_country(df):
    df = get_obj_if_Acc(df)
    dic = {'EL': 'GR',  # needed, as some datasets use for Greece and United K.
           'UK': 'GB'}  # codes that are not conform to ISO 3166-1 alpha2.
    return df.assign(Country=df.Country.replace(dic)
                     .map(country_map.set_index('alpha_2')['name']))


def convert_country_to_alpha2(df):
    df = get_obj_if_Acc(df)
    alpha2 = df.Country.map(country_map.set_index('name')['alpha_2'])\
               .fillna(country_map.dropna(subset=['official_name'])
                       .set_index('official_name')['alpha_2'])
    return df.assign(Country = alpha2)


def breakdown_matches(df):
                        
                        # Function to inspect grouped and matched entries of a matched
    #dataframe. Breaks down to all ingoing data on detailed level.


df : pd.DataFrame
        #Matched data with not empty projectID-column. Keys of projectID must
        #be specified in powerplantmatching.data.data_config
  df = get_obj_if_Acc(df)

    from . import data
    assert('projectID' in df)
    if isinstance(df.projectID.iloc[0], list):
        sources = [df.powerplant.get_name()]
        single_source_b = True
    else :
        sources = df.projectID.apply(list).explode().unique()
        single_source_b = False
    sources = pd.concat(
            [getattr(data, s)().set_index('projectID')
             for s in sources], sort=False)
    if df.index.nlevels > 1:
        stackedIDs = (df['projectID'].stack()
                      .apply(pd.Series).stack()
                      .dropna())
    elif single_source_b:
        stackedIDs = (df['projectID']
                      .apply(pd.Series).stack())
    else:
        stackedIDs = (df['projectID']
                      .apply(pd.Series).stack()
                      .apply(pd.Series).stack()
                      .dropna())
    return (sources
            .reindex(stackedIDs)
            .set_axis(stackedIDs.to_frame('projectID')
                      .set_index('projectID', append=True).droplevel(-2).index,
                      inplace=False)
            .rename_axis(index=['id', 'source', 'projectID']))



def restore_blocks(df, mode=2, config=None):
                        
                        from .data import OPSD
    df = get_obj_if_Acc(df)
    assert('projectID' in df)

    config = get_config() if config is None else config
    


    bd = breakdown_matches(df)
    if mode == 1:
        block_map = (bd.reset_index(['source'])['source'].groupby(level='id')
                     .agg(lambda x: pd.Series(x).mode()[0]))
        blocks_i = pd.MultiIndex.from_frame(block_map.reset_index())
        res = bd.reset_index('projectID').loc[blocks_i].set_index('projectID',
                                                                  append=True)
    elif mode == 2:
        sources = df.projectID.apply(list).explode().unique()
        rel_scores = pd.Series({s: config[s]['reliability_score']
                                for s in sources})\
                       .sort_values(ascending=False)
        res = pd.DataFrame().rename_axis(index='id')
        for s in rel_scores.index:
            subset = bd.reindex(index=[s], level='source')
            subset_i = subset.index.unique('id').difference(res.index.unique('id'))
            res = pd.concat([res, subset.reindex(index=subset_i, level='id')])
    else:
        raise ValueError(f'Given `mode` must be either 1 or 2 but is: {mode}')
        
    res = res.sort_index(level='id').reset_index(level=[0, 1])

    # Now append Block information from OPSD German list:
    df_blocks = (OPSD(rawDE_withBlocks=True)
                 .rename(columns={'name_bnetza': 'Name'}))['Name']
    res.update(df_blocks)
    return res


def parse_Geoposition(location, zipcode='', country='',
                      use_saved_locations=False, saved_only=False):
                        
    
     from geopy.geocoders import GoogleV3  # ArcGIS  Yandex Nominatim
    import geopy.exc

    if location is None or location == float:
        return np.nan

    alpha2 = country_alpha2(country)
    try:
        gdata = (GoogleV3(api_key=get_config()['google_api_key'], timeout=10)
                 .geocode(query=location,
                          components={'country': alpha2,
                                      'postal_code': str(zipcode)},
                          exactly_one=True))
    except geopy.exc.GeocoderQueryError as e:
        logger.warn(e)

    if gdata is not None:
        return pd.Series({'Name': location, 'Country': country,
                          'lat': gdata.latitude, 'lon': gdata.longitude})

def fill_geoposition(df, use_saved_locations=False, saved_only=False):
                        
                    
                
             df = get_obj_if_Acc(df)

    if use_saved_locations and get_config()['google_api_key'] is None:
        logger.warning('Geoparsing not possible as no google api key was '
                       'found, please add the key to your config.yaml if you '
                       'want to enable it.')

    if use_saved_locations:
        locs = pd.read_csv(_package_data('parsed_locations.csv'), index_col=[0, 1])
        df = df.where(df[['lat', 'lon']].notnull().all(1),
                 df.drop(columns=['lat', 'lon'])
                   .join(locs, on=['Name', 'Country']))
    if saved_only: return df

    logger.info("Parse geopositions for missing lat/lon values")
    missing = df.lat.isnull()
    geodata = df[missing].apply(
                lambda ds: parse_Geoposition(ds['Name'], country=ds['Country']),
                axis=1)
    geodata.drop_duplicates(subset=['Name', 'Country'])\
           .set_index(['Name', 'Country'])\
           .to_csv(_package_data('parsed_locations.csv'), mode='a', header=False)

    df.loc[missing, ['lat', 'lon']] = geodata

    return df.reindex(columns=df.columns)
                        